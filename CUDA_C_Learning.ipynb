{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVyjyzrbbKHtMaWRkAWljj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sklow/google-colaboratory/blob/main/CUDA_C_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA C の練習\n",
        "- CUDA C プロフェッショナル プログラミングを使用"
      ],
      "metadata": {
        "id": "d7lykAectq57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "! をつけることで、bashの実行"
      ],
      "metadata": {
        "id": "S3yHE6GKt3UU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONP9__-wmXeb",
        "outputId": "509e3c64-398f-4790-87b1-aab5bb65484d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ],
      "source": [
        "!which nvcc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjAjhmj140xS",
        "outputId": "21fb67ea-e0c9-4521-ad69-6f22a37927cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "google colab 上のjupyter notebook でnvccを使えるようにする"
      ],
      "metadata": {
        "id": "WaXElezjsRDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rSl_twRnYMe",
        "outputId": "153cd984-f5b2-4026-a65f-6e9484b5263e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-7_f_13tj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-7_f_13tj\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4293 sha256=5d76115878664fbf2f56603839e4e13947a2a5daad5a5c2c02e72d2c24257ef1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vpf5fnax/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoT2vsPhsDWT",
        "outputId": "2ab573f3-4220-456e-db59-7a5e615fc13e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA samplesのヘッダーファイル活用.\n",
        "checkCudaErrors関数等があるヘッダーファイル(helper_cuda.h)をインストールする。"
      ],
      "metadata": {
        "id": "xHaZbslgs7iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zw_yLCzsaKF",
        "outputId": "74a1526e-612a-4003-b6bf-fea7d9e07fbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 17263, done.\u001b[K\n",
            "remote: Counting objects: 100% (3243/3243), done.\u001b[K\n",
            "remote: Compressing objects: 100% (361/361), done.\u001b[K\n",
            "remote: Total 17263 (delta 2922), reused 3120 (delta 2882), pack-reused 14020\u001b[K\n",
            "Receiving objects: 100% (17263/17263), 133.10 MiB | 27.26 MiB/s, done.\n",
            "Resolving deltas: 100% (14979/14979), done.\n",
            "Updating files: 100% (3998/3998), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r cuda-samples/Common/* /usr/local/include"
      ],
      "metadata": {
        "id": "ku2H-LxUsqsT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%writefile temp.cpp で ファイル書き込みができる  \n",
        "まず、参考書のcommon.hを用意する"
      ],
      "metadata": {
        "id": "1RtqLSJztQtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /usr/local/common"
      ],
      "metadata": {
        "id": "s7XjjjAzxJdG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /usr/local/common/common.h\n",
        "#include <sys/time.h>\n",
        "\n",
        "#ifndef _COMMON_H\n",
        "#define _COMMON_H\n",
        "\n",
        "#define CHECK(call)                                                            \\\n",
        "{                                                                              \\\n",
        "    cudaError_t error = call;                                            \\\n",
        "    if (error != cudaSuccess)                                                  \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n",
        "        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n",
        "                cudaGetErrorString(error));                                    \\\n",
        "        exit(1);                                                               \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "#define CHECK_CUBLAS(call)                                                     \\\n",
        "{                                                                              \\\n",
        "    cublasStatus_t err;                                                        \\\n",
        "    if ((err = (call)) != CUBLAS_STATUS_SUCCESS)                               \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Got CUBLAS error %d at %s:%d\\n\", err, __FILE__,       \\\n",
        "                __LINE__);                                                     \\\n",
        "        exit(1);                                                               \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "#define CHECK_CURAND(call)                                                     \\\n",
        "{                                                                              \\\n",
        "    curandStatus_t err;                                                        \\\n",
        "    if ((err = (call)) != CURAND_STATUS_SUCCESS)                               \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Got CURAND error %d at %s:%d\\n\", err, __FILE__,       \\\n",
        "                __LINE__);                                                     \\\n",
        "        exit(1);                                                               \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "#define CHECK_CUFFT(call)                                                      \\\n",
        "{                                                                              \\\n",
        "    cufftResult err;                                                           \\\n",
        "    if ( (err = (call)) != CUFFT_SUCCESS)                                      \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Got CUFFT error %d at %s:%d\\n\", err, __FILE__,        \\\n",
        "                __LINE__);                                                     \\\n",
        "        exit(1);                                                               \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "#define CHECK_CUSPARSE(call)                                                   \\\n",
        "{                                                                              \\\n",
        "    cusparseStatus_t err;                                                      \\\n",
        "    if ((err = (call)) != CUSPARSE_STATUS_SUCCESS)                             \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Got error %d at %s:%d\\n\", err, __FILE__, __LINE__);   \\\n",
        "        cudaError_t cuda_err = cudaGetLastError();                             \\\n",
        "        if (cuda_err != cudaSuccess)                                           \\\n",
        "        {                                                                      \\\n",
        "            fprintf(stderr, \"  CUDA error \\\"%s\\\" also detected\\n\",             \\\n",
        "                    cudaGetErrorString(cuda_err));                             \\\n",
        "        }                                                                      \\\n",
        "        exit(1);                                                               \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "inline double seconds()\n",
        "{\n",
        "    struct timeval tp;\n",
        "    struct timezone tzp;\n",
        "    int i = gettimeofday(&tp, &tzp);\n",
        "    return ((double)tp.tv_sec + (double)tp.tv_usec * 1.e-6);\n",
        "}\n",
        "\n",
        "#endif // _COMMON_H\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDT_3I5Wv83R",
        "outputId": "002b1cf2-2340-498f-ba7f-411532e8e7b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /usr/local/common/common.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/local/common/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph7-XgWdtnxp",
        "outputId": "077e0a91-202a-4fa0-c2d4-b523b02b9e16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/usr/local/common/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ここまでは初期設定"
      ],
      "metadata": {
        "id": "Tnr4LA79ecxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"../common/common.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * A simple introduction to programming in CUDA. This program prints \"Hello\n",
        " * World from GPU! from 10 CUDA threads running on the GPU.\n",
        " */\n",
        "\n",
        "__global__ void helloFromGPU()\n",
        "{\n",
        "    printf(\"Hello World from GPU! \\n\");\n",
        "    printf(\"thread: {%d}\\n\", threadIdx.x);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"Hello World from CPU!\\n\");\n",
        "\n",
        "    helloFromGPU<<<1, 10>>>();\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1b7POsTsv1z",
        "outputId": "dd23b238-d95b-4cc4-aa99-eaab40fc9590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from CPU!\n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "Hello World from GPU! \n",
            "thread: {0}\n",
            "thread: {1}\n",
            "thread: {2}\n",
            "thread: {3}\n",
            "thread: {4}\n",
            "thread: {5}\n",
            "thread: {6}\n",
            "thread: {7}\n",
            "thread: {8}\n",
            "thread: {9}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * Display the dimensionality of a thread block and grid from the host and\n",
        " * device.\n",
        " */\n",
        "\n",
        "__global__ void checkIndex(void)\n",
        "{\n",
        "    printf(\"threadIdx:(%d, %d, %d)\\tblockIdx:(%d, %d, %d)\\tblockDim:(%d, %d, %d)\\tgridDim:(%d, %d, %d)\\n\", threadIdx.x, threadIdx.y, threadIdx.z,blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x, gridDim.y, gridDim.z);\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // define total data element\n",
        "    int nElem = 6;\n",
        "\n",
        "    // define grid and block structure\n",
        "    dim3 block(3);\n",
        "    dim3 grid((nElem + block.x - 1) / block.x);\n",
        "\n",
        "    // check grid and block dimension from host side\n",
        "    printf(\"grid.x %d grid.y %d grid.z %d\\n\", grid.x, grid.y, grid.z);\n",
        "    printf(\"block.x %d block.y %d block.z %d\\n\", block.x, block.y, block.z);\n",
        "\n",
        "    // check grid and block dimension from device side\n",
        "    checkIndex<<<grid, block>>>();\n",
        "\n",
        "    // reset device before you leave\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return(0);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loWfTMHWkoje",
        "outputId": "4a6af14d-0e26-46e4-f40f-77e9b3f34b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid.x 2 grid.y 1 grid.z 1\n",
            "block.x 3 block.y 1 block.z 1\n",
            "threadIdx:(0, 0, 0)\tblockIdx:(0, 0, 0)\tblockDim:(3, 1, 1)\tgridDim:(2, 1, 1)\n",
            "threadIdx:(1, 0, 0)\tblockIdx:(0, 0, 0)\tblockDim:(3, 1, 1)\tgridDim:(2, 1, 1)\n",
            "threadIdx:(2, 0, 0)\tblockIdx:(0, 0, 0)\tblockDim:(3, 1, 1)\tgridDim:(2, 1, 1)\n",
            "threadIdx:(0, 0, 0)\tblockIdx:(1, 0, 0)\tblockDim:(3, 1, 1)\tgridDim:(2, 1, 1)\n",
            "threadIdx:(1, 0, 0)\tblockIdx:(1, 0, 0)\tblockDim:(3, 1, 1)\tgridDim:(2, 1, 1)\n",
            "threadIdx:(2, 0, 0)\tblockIdx:(1, 0, 0)\tblockDim:(3, 1, 1)\tgridDim:(2, 1, 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. Only a single thread block is used in this small case, for simplicity.\n",
        " * sumArraysOnHost sequentially iterates through vector elements on the host.\n",
        " */\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"Arrays do not match!\\n\");\n",
        "            printf(\"host %5.2f gpu %5.2f at current %d\\n\", hostRef[i],\n",
        "                   gpuRef[i], i);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match) printf(\"Arrays match.\\n\\n\");\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void initialData(float *ip, int size)\n",
        "{\n",
        "    // generate different seed for random number\n",
        "    time_t t;\n",
        "    srand((unsigned) time(&t));\n",
        "\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)(rand() & 0xFF) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void sumArraysOnHost(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    for (int idx = 0; idx < N; idx++)\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "}\n",
        "\n",
        "__global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    int i = threadIdx.x;\n",
        "\n",
        "    //if (i < N)\n",
        "    C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of vectors\n",
        "    int nElem = 1 << 5;\n",
        "    printf(\"Vector size %d\\n\", nElem);\n",
        "\n",
        "    // malloc host memory\n",
        "    size_t nBytes = nElem * sizeof(float);\n",
        "\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A     = (float *)malloc(nBytes);\n",
        "    h_B     = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef  = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    initialData(h_A, nElem);\n",
        "    initialData(h_B, nElem);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef,  0, nBytes);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_B, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    dim3 block (nElem);\n",
        "    dim3 grid  (1);\n",
        "\n",
        "    sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);\n",
        "    printf(\"Execution configure <<<%d, %d>>>\\n\", grid.x, block.x);\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // add vector at host side for result checks\n",
        "    sumArraysOnHost(h_A, h_B, hostRef, nElem);\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nElem);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_A));\n",
        "    CHECK(cudaFree(d_B));\n",
        "    CHECK(cudaFree(d_C));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    CHECK(cudaDeviceReset());\n",
        "    return(0);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOTxsdLfoKVY",
        "outputId": "bba46b00-8de4-4975-e157-e70bc56130a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpj6ng_09h/55fd04e0-bc0a-456e-b09e-e42eb3a62a1e.out Starting...\n",
            "Vector size 32\n",
            "Execution configure <<<1, 32>>>\n",
            "Arrays match.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile timer.cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include<math.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. Only a single thread block is used in this small case, for simplicity.\n",
        " * sumArraysOnHost sequentially iterates through vector elements on the host.\n",
        " * This version of sumArrays adds host timers to measure GPU and CPU\n",
        " * performance.\n",
        " */\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"Arrays do not match!\\n\");\n",
        "            printf(\"host %5.2f gpu %5.2f at current %d\\n\", hostRef[i],\n",
        "                   gpuRef[i], i);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match) printf(\"Arrays match.\\n\\n\");\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void initialData(float *ip, int size)\n",
        "{\n",
        "    // generate different seed for random number\n",
        "    time_t t;\n",
        "    srand((unsigned) time(&t));\n",
        "\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumArraysOnHost(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    for (int idx = 0; idx < N; idx++)\n",
        "    {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "__global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < N) C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of vectors\n",
        "    int nElem = 1 << 24;\n",
        "    printf(\"Vector size %d\\n\", nElem);\n",
        "\n",
        "    // malloc host memory\n",
        "    size_t nBytes = nElem * sizeof(float);\n",
        "\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A     = (float *)malloc(nBytes);\n",
        "    h_B     = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef  = (float *)malloc(nBytes);\n",
        "\n",
        "    double iStart, iElaps;\n",
        "\n",
        "    // initialize data at host side\n",
        "    iStart = seconds();\n",
        "    initialData(h_A, nElem);\n",
        "    initialData(h_B, nElem);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"initialData Time elapsed %f sec\\n\", iElaps);\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef,  0, nBytes);\n",
        "\n",
        "    // add vector at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumArraysOnHost(h_A, h_B, hostRef, nElem);\n",
        "    float cpuElaps = seconds() - iStart;\n",
        "    printf(\"sumArraysOnHost Time elapsed %f sec\\n\", cpuElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_B, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "\n",
        "    for(int i=1; i<2; i++)\n",
        "    {\n",
        "        int iLen = atoi(argv[1]);\n",
        "        dim3 block (iLen);\n",
        "        dim3 grid  ((nElem + block.x - 1) / block.x);\n",
        "\n",
        "        iStart = seconds();\n",
        "        sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);\n",
        "        CHECK(cudaDeviceSynchronize());\n",
        "        float gpuElaps = seconds() - iStart;\n",
        "        printf(\"sumArraysOnGPU <<<  %d, %d  >>>  Time elapsed %f sec\\n\", grid.x,\n",
        "           block.x, gpuElaps);\n",
        "\n",
        "        printf(\"speed scale (gpu/cpu) %f \\n\", (float)(cpuElaps/gpuElaps));\n",
        "    }\n",
        "\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError()) ;\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nElem);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_A));\n",
        "    CHECK(cudaFree(d_B));\n",
        "    CHECK(cudaFree(d_C));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    return(0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_bEHCUEuutz",
        "outputId": "c5691aac-4909-42ca-f0a8-9d84151256de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting timer.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc timer.cu -o timer"
      ],
      "metadata": {
        "id": "szJISGpV9u2I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./timer 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alGJ8S8N-T_b",
        "outputId": "661ed167-e1eb-42fa-f8b4-eeaea4b4a551"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./timer Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Vector size 16777216\n",
            "initialData Time elapsed 0.666355 sec\n",
            "sumArraysOnHost Time elapsed 0.054030 sec\n",
            "sumArraysOnGPU <<<  16384, 1024  >>>  Time elapsed 0.001061 sec\n",
            "speed scale (gpu/cpu) 50.925392 \n",
            "Arrays match.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./timer 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIeL_Xa6ZwBP",
        "outputId": "8637d895-0168-4a55-f5be-8933d59af57e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./timer Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Vector size 16777216\n",
            "initialData Time elapsed 0.724029 sec\n",
            "sumArraysOnHost Time elapsed 0.051387 sec\n",
            "sumArraysOnGPU <<<  32768, 512  >>>  Time elapsed 0.001027 sec\n",
            "speed scale (gpu/cpu) 50.030872 \n",
            "Arrays match.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./timer 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsv_FWTjZ5jd",
        "outputId": "09e87265-dc6c-4128-bbdf-189efbe1343d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./timer Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Vector size 16777216\n",
            "initialData Time elapsed 0.815801 sec\n",
            "sumArraysOnHost Time elapsed 0.062845 sec\n",
            "sumArraysOnGPU <<<  65536, 256  >>>  Time elapsed 0.001054 sec\n",
            "speed scale (gpu/cpu) 59.622482 \n",
            "Arrays match.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./timer 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqy5Jz5EaJ00",
        "outputId": "15450d87-5b2e-4d45-83b0-bc288d3eccd5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./timer Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Vector size 16777216\n",
            "initialData Time elapsed 0.668446 sec\n",
            "sumArraysOnHost Time elapsed 0.053728 sec\n",
            "sumArraysOnGPU <<<  131072, 128  >>>  Time elapsed 0.000985 sec\n",
            "speed scale (gpu/cpu) 54.551441 \n",
            "Arrays match.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./timer 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqhfnQhxaTPQ",
        "outputId": "72114f66-a61e-4b6c-9237-03b65a7bad4f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./timer Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Vector size 16777216\n",
            "initialData Time elapsed 0.685062 sec\n",
            "sumArraysOnHost Time elapsed 0.056477 sec\n",
            "sumArraysOnGPU <<<  262144, 64  >>>  Time elapsed 0.001031 sec\n",
            "speed scale (gpu/cpu) 54.770405 \n",
            "Arrays match.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile timer.cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include<math.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. Only a single thread block is used in this small case, for simplicity.\n",
        " * sumArraysOnHost sequentially iterates through vector elements on the host.\n",
        " * This version of sumArrays adds host timers to measure GPU and CPU\n",
        " * performance.\n",
        " */\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"Arrays do not match!\\n\");\n",
        "            printf(\"host %5.2f gpu %5.2f at current %d\\n\", hostRef[i],\n",
        "                   gpuRef[i], i);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match) printf(\"Arrays match.\\n\\n\");\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void initialData(float *ip, int size)\n",
        "{\n",
        "    // generate different seed for random number\n",
        "    time_t t;\n",
        "    srand((unsigned) time(&t));\n",
        "\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumArraysOnHost(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    for (int idx = 0; idx < N; idx++)\n",
        "    {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "__global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < N) C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of vectors\n",
        "    int nElem = 1 << 24;\n",
        "    printf(\"Vector size %d\\n\", nElem);\n",
        "\n",
        "    // malloc host memory\n",
        "    size_t nBytes = nElem * sizeof(float);\n",
        "\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A     = (float *)malloc(nBytes);\n",
        "    h_B     = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef  = (float *)malloc(nBytes);\n",
        "\n",
        "    double iStart, iElaps;\n",
        "\n",
        "    // initialize data at host side\n",
        "    iStart = seconds();\n",
        "    initialData(h_A, nElem);\n",
        "    initialData(h_B, nElem);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"initialData Time elapsed %f sec\\n\", iElaps);\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef,  0, nBytes);\n",
        "\n",
        "    // add vector at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumArraysOnHost(h_A, h_B, hostRef, nElem);\n",
        "    float cpuElaps = seconds() - iStart;\n",
        "    printf(\"sumArraysOnHost Time elapsed %f sec\\n\", cpuElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_B, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "\n",
        "    for(int i=1; i<2; i++)\n",
        "    {\n",
        "        int iLen = 512;\n",
        "        dim3 block (iLen);\n",
        "        dim3 grid  ((nElem + block.x - 1) / block.x);\n",
        "\n",
        "        iStart = seconds();\n",
        "        sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);\n",
        "        CHECK(cudaDeviceSynchronize());\n",
        "        float gpuElaps = seconds() - iStart;\n",
        "        printf(\"sumArraysOnGPU <<<  %d, %d  >>>  Time elapsed %f sec\\n\", grid.x,\n",
        "           block.x, gpuElaps);\n",
        "\n",
        "        printf(\"speed scale (gpu/cpu) %f \\n\", (float)(cpuElaps/gpuElaps));\n",
        "    }\n",
        "\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError()) ;\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nElem);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_A));\n",
        "    CHECK(cudaFree(d_B));\n",
        "    CHECK(cudaFree(d_C));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    return(0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqocppViXZpm",
        "outputId": "dcdbff1f-25bf-46b6-bb17-ecf8e172dd1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting timer.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc timer.cu -o timer && nvprof ./timer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1N8nbTTXl2k",
        "outputId": "00183d1a-ba73-447b-f333-2a97c706510e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./timer Starting...\n",
            "==4870== NVPROF is profiling process 4870, command: ./timer\n",
            "Using Device 0: Tesla T4\n",
            "Vector size 16777216\n",
            "initialData Time elapsed 0.665295 sec\n",
            "sumArraysOnHost Time elapsed 0.051800 sec\n",
            "sumArraysOnGPU <<<  32768, 512  >>>  Time elapsed 0.001107 sec\n",
            "speed scale (gpu/cpu) 46.794098 \n",
            "Arrays match.\n",
            "\n",
            "==4870== Profiling application: ./timer\n",
            "==4870== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   74.59%  42.187ms         3  14.062ms  13.953ms  14.221ms  [CUDA memcpy HtoD]\n",
            "                   23.98%  13.565ms         1  13.565ms  13.565ms  13.565ms  [CUDA memcpy DtoH]\n",
            "                    1.42%  803.44us         1  803.44us  803.44us  803.44us  sumArraysOnGPU(float*, float*, float*, int)\n",
            "      API calls:   61.27%  98.537ms         1  98.537ms  98.537ms  98.537ms  cudaSetDevice\n",
            "                   35.30%  56.769ms         4  14.192ms  13.921ms  14.417ms  cudaMemcpy\n",
            "                    2.19%  3.5245ms         3  1.1748ms  264.10us  2.1343ms  cudaFree\n",
            "                    0.50%  806.64us         1  806.64us  806.64us  806.64us  cudaDeviceSynchronize\n",
            "                    0.41%  658.16us         3  219.39us  129.66us  376.01us  cudaMalloc\n",
            "                    0.17%  279.94us         1  279.94us  279.94us  279.94us  cudaLaunchKernel\n",
            "                    0.08%  132.15us       114  1.1590us     137ns  52.354us  cuDeviceGetAttribute\n",
            "                    0.06%  90.944us         1  90.944us  90.944us  90.944us  cudaGetDeviceProperties\n",
            "                    0.01%  11.378us         1  11.378us  11.378us  11.378us  cuDeviceGetName\n",
            "                    0.00%  5.1890us         1  5.1890us  5.1890us  5.1890us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.5070us         1  4.5070us  4.5070us  4.5070us  cuDeviceTotalMem\n",
            "                    0.00%  1.5430us         3     514ns     175ns  1.1120us  cuDeviceGetCount\n",
            "                    0.00%  1.1980us         2     599ns     179ns  1.0190us  cuDeviceGet\n",
            "                    0.00%     668ns         1     668ns     668ns     668ns  cudaGetLastError\n",
            "                    0.00%     607ns         1     607ns     607ns     607ns  cuModuleGetLoadingMode\n",
            "                    0.00%     217ns         1     217ns     217ns     217ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile timer.cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include<math.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. Only a single thread block is used in this small case, for simplicity.\n",
        " * sumArraysOnHost sequentially iterates through vector elements on the host.\n",
        " * This version of sumArrays adds host timers to measure GPU and CPU\n",
        " * performance.\n",
        " */\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"Arrays do not match!\\n\");\n",
        "            printf(\"host %5.2f gpu %5.2f at current %d\\n\", hostRef[i],\n",
        "                   gpuRef[i], i);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match) printf(\"Arrays match.\\n\\n\");\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void initialData(float *ip, int size)\n",
        "{\n",
        "    // generate different seed for random number\n",
        "    time_t t;\n",
        "    srand((unsigned) time(&t));\n",
        "\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumArraysOnHost(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    for (int idx = 0; idx < N; idx++)\n",
        "    {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "__global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < N) C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of vectors\n",
        "    int nElem = 1 << 24;\n",
        "    printf(\"Vector size %d\\n\", nElem);\n",
        "\n",
        "    // malloc host memory\n",
        "    size_t nBytes = nElem * sizeof(float);\n",
        "\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A     = (float *)malloc(nBytes);\n",
        "    h_B     = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef  = (float *)malloc(nBytes);\n",
        "\n",
        "    double iStart, iElaps;\n",
        "\n",
        "    // initialize data at host side\n",
        "    iStart = seconds();\n",
        "    initialData(h_A, nElem);\n",
        "    initialData(h_B, nElem);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"initialData Time elapsed %f sec\\n\", iElaps);\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef,  0, nBytes);\n",
        "\n",
        "    // add vector at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumArraysOnHost(h_A, h_B, hostRef, nElem);\n",
        "    float cpuElaps = seconds() - iStart;\n",
        "    printf(\"sumArraysOnHost Time elapsed %f sec\\n\", cpuElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_B, nBytes));\n",
        "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "\n",
        "    for(int i=1; i<2; i++)\n",
        "    {\n",
        "        int iLen = 256;\n",
        "        dim3 block (iLen);\n",
        "        dim3 grid  ((nElem + block.x - 1) / block.x);\n",
        "\n",
        "        iStart = seconds();\n",
        "        sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);\n",
        "        CHECK(cudaDeviceSynchronize());\n",
        "        float gpuElaps = seconds() - iStart;\n",
        "        printf(\"sumArraysOnGPU <<<  %d, %d  >>>  Time elapsed %f sec\\n\", grid.x,\n",
        "           block.x, gpuElaps);\n",
        "\n",
        "        printf(\"speed scale (gpu/cpu) %f \\n\", (float)(cpuElaps/gpuElaps));\n",
        "    }\n",
        "\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError()) ;\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nElem);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_A));\n",
        "    CHECK(cudaFree(d_B));\n",
        "    CHECK(cudaFree(d_C));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    return(0);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NHXjQQCYLqv",
        "outputId": "225a7e99-3e58-4bad-8d9c-dff7275e2884"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting timer.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-vVvBHLYf-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example helps to visualize the relationship between thread/block IDs and\n",
        " * offsets into data. For each CUDA thread, this example displays the\n",
        " * intra-block thread ID, the inter-block block ID, the global coordinate of a\n",
        " * thread, the calculated offset into input data, and the input data at that\n",
        " * offset.\n",
        " */\n",
        "\n",
        "void printMatrix(int *C, const int nx, const int ny)\n",
        "{\n",
        "    int *ic = C;\n",
        "    printf(\"\\nMatrix: (%d.%d)\\n\", nx, ny);\n",
        "\n",
        "    for (int iy = 0; iy < ny; iy++)\n",
        "    {\n",
        "        for (int ix = 0; ix < nx; ix++)\n",
        "        {\n",
        "            printf(\"%3d\", ic[ix]);\n",
        "\n",
        "        }\n",
        "\n",
        "        ic += nx;\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\n\");\n",
        "    return;\n",
        "}\n",
        "\n",
        "__global__ void printThreadIndex(int *A, const int nx, const int ny)\n",
        "{\n",
        "    int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int iy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    unsigned int idx = iy * nx + ix;\n",
        "\n",
        "    printf(\"thread_id (%d,%d) block_id (%d,%d) coordinate (%d,%d) global index\"\n",
        "           \" %2d ival %2d\\n\", threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y,\n",
        "           ix, iy, idx, A[idx]);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // get device information\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set matrix dimension\n",
        "    int nx = 8;\n",
        "    int ny = 6;\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "\n",
        "    // malloc host memory\n",
        "    int *h_A;\n",
        "    h_A = (int *)malloc(nBytes);\n",
        "\n",
        "    // iniitialize host matrix with integer\n",
        "    for (int i = 0; i < nxy; i++)\n",
        "    {\n",
        "        h_A[i] = i;\n",
        "    }\n",
        "    printMatrix(h_A, nx, ny);\n",
        "\n",
        "    // malloc device memory\n",
        "    int *d_MatA;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // set up execution configuration\n",
        "    dim3 block(4, 2);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    // invoke the kernel\n",
        "    printThreadIndex<<<grid, block>>>(d_MatA, nx, ny);\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // free host and devide memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    free(h_A);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTVjM1XXl3t1",
        "outputId": "59bf2e26-717d-45e5-c660-a349f838ea3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpw55ey9pl/79608879-5ff5-49e7-b933-a08eacb60a84.out Starting...\n",
            "Using Device 0: Tesla T4\n",
            "\n",
            "Matrix: (8.6)\n",
            "  0  1  2  3  4  5  6  7\n",
            "  8  9 10 11 12 13 14 15\n",
            " 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31\n",
            " 32 33 34 35 36 37 38 39\n",
            " 40 41 42 43 44 45 46 47\n",
            "\n",
            "thread_id (0,0) block_id (0,1) coordinate (0,2) global index 16 ival 16\n",
            "thread_id (1,0) block_id (0,1) coordinate (1,2) global index 17 ival 17\n",
            "thread_id (2,0) block_id (0,1) coordinate (2,2) global index 18 ival 18\n",
            "thread_id (3,0) block_id (0,1) coordinate (3,2) global index 19 ival 19\n",
            "thread_id (0,1) block_id (0,1) coordinate (0,3) global index 24 ival 24\n",
            "thread_id (1,1) block_id (0,1) coordinate (1,3) global index 25 ival 25\n",
            "thread_id (2,1) block_id (0,1) coordinate (2,3) global index 26 ival 26\n",
            "thread_id (3,1) block_id (0,1) coordinate (3,3) global index 27 ival 27\n",
            "thread_id (0,0) block_id (1,2) coordinate (4,4) global index 36 ival 36\n",
            "thread_id (1,0) block_id (1,2) coordinate (5,4) global index 37 ival 37\n",
            "thread_id (2,0) block_id (1,2) coordinate (6,4) global index 38 ival 38\n",
            "thread_id (3,0) block_id (1,2) coordinate (7,4) global index 39 ival 39\n",
            "thread_id (0,1) block_id (1,2) coordinate (4,5) global index 44 ival 44\n",
            "thread_id (1,1) block_id (1,2) coordinate (5,5) global index 45 ival 45\n",
            "thread_id (2,1) block_id (1,2) coordinate (6,5) global index 46 ival 46\n",
            "thread_id (3,1) block_id (1,2) coordinate (7,5) global index 47 ival 47\n",
            "thread_id (0,0) block_id (0,0) coordinate (0,0) global index  0 ival  0\n",
            "thread_id (1,0) block_id (0,0) coordinate (1,0) global index  1 ival  1\n",
            "thread_id (2,0) block_id (0,0) coordinate (2,0) global index  2 ival  2\n",
            "thread_id (3,0) block_id (0,0) coordinate (3,0) global index  3 ival  3\n",
            "thread_id (0,1) block_id (0,0) coordinate (0,1) global index  8 ival  8\n",
            "thread_id (1,1) block_id (0,0) coordinate (1,1) global index  9 ival  9\n",
            "thread_id (2,1) block_id (0,0) coordinate (2,1) global index 10 ival 10\n",
            "thread_id (3,1) block_id (0,0) coordinate (3,1) global index 11 ival 11\n",
            "thread_id (0,0) block_id (0,2) coordinate (0,4) global index 32 ival 32\n",
            "thread_id (1,0) block_id (0,2) coordinate (1,4) global index 33 ival 33\n",
            "thread_id (2,0) block_id (0,2) coordinate (2,4) global index 34 ival 34\n",
            "thread_id (3,0) block_id (0,2) coordinate (3,4) global index 35 ival 35\n",
            "thread_id (0,1) block_id (0,2) coordinate (0,5) global index 40 ival 40\n",
            "thread_id (1,1) block_id (0,2) coordinate (1,5) global index 41 ival 41\n",
            "thread_id (2,1) block_id (0,2) coordinate (2,5) global index 42 ival 42\n",
            "thread_id (3,1) block_id (0,2) coordinate (3,5) global index 43 ival 43\n",
            "thread_id (0,0) block_id (1,1) coordinate (4,2) global index 20 ival 20\n",
            "thread_id (1,0) block_id (1,1) coordinate (5,2) global index 21 ival 21\n",
            "thread_id (2,0) block_id (1,1) coordinate (6,2) global index 22 ival 22\n",
            "thread_id (3,0) block_id (1,1) coordinate (7,2) global index 23 ival 23\n",
            "thread_id (0,1) block_id (1,1) coordinate (4,3) global index 28 ival 28\n",
            "thread_id (1,1) block_id (1,1) coordinate (5,3) global index 29 ival 29\n",
            "thread_id (2,1) block_id (1,1) coordinate (6,3) global index 30 ival 30\n",
            "thread_id (3,1) block_id (1,1) coordinate (7,3) global index 31 ival 31\n",
            "thread_id (0,0) block_id (1,0) coordinate (4,0) global index  4 ival  4\n",
            "thread_id (1,0) block_id (1,0) coordinate (5,0) global index  5 ival  5\n",
            "thread_id (2,0) block_id (1,0) coordinate (6,0) global index  6 ival  6\n",
            "thread_id (3,0) block_id (1,0) coordinate (7,0) global index  7 ival  7\n",
            "thread_id (0,1) block_id (1,0) coordinate (4,1) global index 12 ival 12\n",
            "thread_id (1,1) block_id (1,0) coordinate (5,1) global index 13 ival 13\n",
            "thread_id (2,1) block_id (1,0) coordinate (6,1) global index 14 ival 14\n",
            "thread_id (3,1) block_id (1,0) coordinate (7,1) global index 15 ival 15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. A 2D thread block and 2D grid are used. sumArraysOnHost sequentially\n",
        " * iterates through vector elements on the host.\n",
        " */\n",
        "\n",
        "void initialData(float *ip, const int size)\n",
        "{\n",
        "    int i;\n",
        "\n",
        "    for(i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)(rand() & 0xFF) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx,\n",
        "                     const int ny)\n",
        "{\n",
        "    float *ia = A;\n",
        "    float *ib = B;\n",
        "    float *ic = C;\n",
        "\n",
        "    for (int iy = 0; iy < ny; iy++)\n",
        "    {\n",
        "        for (int ix = 0; ix < nx; ix++)\n",
        "        {\n",
        "            ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "        }\n",
        "\n",
        "        ia += nx;\n",
        "        ib += nx;\n",
        "        ic += nx;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match)\n",
        "        printf(\"Arrays match.\\n\\n\");\n",
        "    else\n",
        "        printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 2D block 2D\n",
        "__global__ void sumMatrixOnGPU2D(float *MatA, float *MatB, float *MatC, int nx,\n",
        "                                 int ny)\n",
        "{\n",
        "    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    unsigned int idx = iy * nx + ix;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "        MatC[idx] = MatA[idx] + MatB[idx];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx = 1 << 14;\n",
        "    int ny = 1 << 14;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "    // malloc host memory\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (float *)malloc(nBytes);\n",
        "    h_B = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "    printf(\"Matrix initialization elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnHost elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 32;\n",
        "    int dimy = 16;\n",
        "    dim3 block(dimx, dimy);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnGPU2D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnGPU2D <<<(%d,%d), (%d,%d)>>> elapsed %f sec\\n\", grid.x,\n",
        "           grid.y,\n",
        "           block.x, block.y, iElaps);\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFS6Ao6DoeJ9",
        "outputId": "4c64b867-6fc4-44cc-c333-9917a8bd565d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmp9zqnjug_/d98fb41d-512e-4341-a06a-cd87d0e24099.out Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Matrix size: nx 16384 ny 16384\n",
            "Matrix initialization elapsed 11.653740 sec\n",
            "sumMatrixOnHost elapsed 0.941889 sec\n",
            "sumMatrixOnGPU2D <<<(512,1024), (32,16)>>> elapsed 0.012913 sec\n",
            "Arrays match.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. A 1D thread block and 1D grid are used. sumArraysOnHost sequentially\n",
        " * iterates through vector elements on the host.\n",
        " */\n",
        "\n",
        "void initialData(float *ip, const int size)\n",
        "{\n",
        "    int i;\n",
        "\n",
        "    for(i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)(rand() & 0xFF ) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx,\n",
        "                     const int ny)\n",
        "{\n",
        "    float *ia = A;\n",
        "    float *ib = B;\n",
        "    float *ic = C;\n",
        "\n",
        "    for (int iy = 0; iy < ny; iy++)\n",
        "    {\n",
        "        for (int ix = 0; ix < nx; ix++)\n",
        "        {\n",
        "            ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "        }\n",
        "\n",
        "        ia += nx;\n",
        "        ib += nx;\n",
        "        ic += nx;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match)\n",
        "        printf(\"Arrays match.\\n\\n\");\n",
        "    else\n",
        "        printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 1D block 1D\n",
        "__global__ void sumMatrixOnGPU1D(float *MatA, float *MatB, float *MatC, int nx,\n",
        "                                 int ny)\n",
        "{\n",
        "    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    if (ix < nx )\n",
        "        for (int iy = 0; iy < ny; iy++)\n",
        "        {\n",
        "            int idx = iy * nx + ix;\n",
        "            MatC[idx] = MatA[idx] + MatB[idx];\n",
        "        }\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx = 1 << 14;\n",
        "    int ny = 1 << 14;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "    // malloc host memory\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (float *)malloc(nBytes);\n",
        "    h_B = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "    printf(\"initialize matrix elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnHost elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 128;\n",
        "    dim3 block(dimx, 1);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, 1);\n",
        "\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnGPU1D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnGPU1D <<<(%d,%d), (%d,%d)>>> elapsed %f sec\\n\", grid.x,\n",
        "           grid.y,\n",
        "           block.x, block.y, iElaps);\n",
        "\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjZquyuPqvcR",
        "outputId": "a67f276b-b6e1-4acf-e005-d0aa2888343a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmp_4iqo0gr/6a29d5e6-276a-4a25-ba01-862142ab046a.out Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Matrix size: nx 16384 ny 16384\n",
            "initialize matrix elapsed 14.306657 sec\n",
            "sumMatrixOnHost elapsed 0.868265 sec\n",
            "sumMatrixOnGPU1D <<<(128,1), (128,1)>>> elapsed 0.018929 sec\n",
            "Arrays match.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. A 1D thread block and 2D grid are used. sumArraysOnHost sequentially\n",
        " * iterates through vector elements on the host.\n",
        " */\n",
        "\n",
        "void initialData(float *ip, const int size)\n",
        "{\n",
        "    int i;\n",
        "\n",
        "    for(i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)(rand() & 0xFF) / 10.0f;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx,\n",
        "                     const int ny)\n",
        "{\n",
        "    float *ia = A;\n",
        "    float *ib = B;\n",
        "    float *ic = C;\n",
        "\n",
        "    for (int iy = 0; iy < ny; iy++)\n",
        "    {\n",
        "        for (int ix = 0; ix < nx; ix++)\n",
        "        {\n",
        "            ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "        }\n",
        "\n",
        "        ia += nx;\n",
        "        ib += nx;\n",
        "        ic += nx;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match)\n",
        "        printf(\"Arrays match.\\n\\n\");\n",
        "    else\n",
        "        printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 2D block 1D\n",
        "__global__ void sumMatrixOnGPUMix(float *MatA, float *MatB, float *MatC, int nx,\n",
        "                                  int ny)\n",
        "{\n",
        "    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    unsigned int iy = blockIdx.y;\n",
        "    unsigned int idx = iy * nx + ix;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "        MatC[idx] = MatA[idx] + MatB[idx];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx = 1 << 14;\n",
        "    int ny = 1 << 14;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "    // malloc host memory\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (float *)malloc(nBytes);\n",
        "    h_B = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "    printf(\"Matrix initialization elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnHost elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 256;\n",
        "    dim3 block(dimx, 1);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, ny);\n",
        "\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnGPUMix<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnGPU2D <<<(%d,%d), (%d,%d)>>> elapsed %f sec\\n\", grid.x,\n",
        "           grid.y,\n",
        "           block.x, block.y, iElaps);\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3MA9tYOriOk",
        "outputId": "7a3f6c08-8530-4d91-8d92-1889f40b947c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmp0m0wz8nk/6eeaf19b-5d2e-424f-9c95-6abdcdc8d7db.out Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Matrix size: nx 16384 ny 16384\n",
            "Matrix initialization elapsed 11.583981 sec\n",
            "sumMatrixOnHost elapsed 0.866551 sec\n",
            "sumMatrixOnGPU2D <<<(64,16384), (256,1)>>> elapsed 0.012345 sec\n",
            "Arrays match.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * Display a variety of information on the first CUDA device in this system,\n",
        " * including driver version, runtime version, compute capability, bytes of\n",
        " * global memory, etc.\n",
        " */\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    int deviceCount = 0;\n",
        "    cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "    if (deviceCount == 0)\n",
        "    {\n",
        "        printf(\"There are no available device(s) that support CUDA\\n\");\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
        "    }\n",
        "\n",
        "    int dev = 0, driverVersion = 0, runtimeVersion = 0;\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Device %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "\n",
        "    cudaDriverGetVersion(&driverVersion);\n",
        "    cudaRuntimeGetVersion(&runtimeVersion);\n",
        "    printf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
        "           driverVersion / 1000, (driverVersion % 100) / 10,\n",
        "           runtimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
        "    printf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
        "           deviceProp.major, deviceProp.minor);\n",
        "    printf(\"  Total amount of global memory:                 %.2f GBytes (%llu \"\n",
        "           \"bytes)\\n\", (float)deviceProp.totalGlobalMem / pow(1024.0, 3),\n",
        "           (unsigned long long)deviceProp.totalGlobalMem);\n",
        "    printf(\"  GPU Clock rate:                                %.0f MHz (%0.2f \"\n",
        "           \"GHz)\\n\", deviceProp.clockRate * 1e-3f,\n",
        "           deviceProp.clockRate * 1e-6f);\n",
        "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
        "           deviceProp.memoryClockRate * 1e-3f);\n",
        "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
        "           deviceProp.memoryBusWidth);\n",
        "\n",
        "    if (deviceProp.l2CacheSize)\n",
        "    {\n",
        "        printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
        "               deviceProp.l2CacheSize);\n",
        "    }\n",
        "\n",
        "    printf(\"  Max Texture Dimension Size (x,y,z)             1D=(%d), \"\n",
        "           \"2D=(%d,%d), 3D=(%d,%d,%d)\\n\", deviceProp.maxTexture1D,\n",
        "           deviceProp.maxTexture2D[0], deviceProp.maxTexture2D[1],\n",
        "           deviceProp.maxTexture3D[0], deviceProp.maxTexture3D[1],\n",
        "           deviceProp.maxTexture3D[2]);\n",
        "    printf(\"  Max Layered Texture Size (dim) x layers        1D=(%d) x %d, \"\n",
        "           \"2D=(%d,%d) x %d\\n\", deviceProp.maxTexture1DLayered[0],\n",
        "           deviceProp.maxTexture1DLayered[1], deviceProp.maxTexture2DLayered[0],\n",
        "           deviceProp.maxTexture2DLayered[1],\n",
        "           deviceProp.maxTexture2DLayered[2]);\n",
        "    printf(\"  Total amount of constant memory:               %lu bytes\\n\",\n",
        "           deviceProp.totalConstMem);\n",
        "    printf(\"  Total amount of shared memory per block:       %lu bytes\\n\",\n",
        "           deviceProp.sharedMemPerBlock);\n",
        "    printf(\"  Total number of registers available per block: %d\\n\",\n",
        "           deviceProp.regsPerBlock);\n",
        "    printf(\"  Warp size:                                     %d\\n\",\n",
        "           deviceProp.warpSize);\n",
        "    printf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n",
        "           deviceProp.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  Maximum number of threads per block:           %d\\n\",\n",
        "           deviceProp.maxThreadsPerBlock);\n",
        "    printf(\"  Maximum sizes of each dimension of a block:    %d x %d x %d\\n\",\n",
        "           deviceProp.maxThreadsDim[0],\n",
        "           deviceProp.maxThreadsDim[1],\n",
        "           deviceProp.maxThreadsDim[2]);\n",
        "    printf(\"  Maximum sizes of each dimension of a grid:     %d x %d x %d\\n\",\n",
        "           deviceProp.maxGridSize[0],\n",
        "           deviceProp.maxGridSize[1],\n",
        "           deviceProp.maxGridSize[2]);\n",
        "    printf(\"  Maximum memory pitch:                          %lu bytes\\n\",\n",
        "           deviceProp.memPitch);\n",
        "\n",
        "    exit(EXIT_SUCCESS);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzdge10dsd06",
        "outputId": "88889bfb-81d2-40cb-8307-b4e68f68a7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmp8wqd5dw_/127fe5d9-f01a-44e8-bb35-69567d3e97bf.out Starting...\n",
            "Detected 1 CUDA Capable device(s)\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.0 / 11.8\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 14.75 GBytes (15835398144 bytes)\n",
            "  GPU Clock rate:                                1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Max Texture Dimension Size (x,y,z)             1D=(131072), 2D=(131072,65536), 3D=(16384,16384,16384)\n",
            "  Max Layered Texture Size (dim) x layers        1D=(32768) x 2048, 2D=(32768,32768) x 2048\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Maximum sizes of each dimension of a block:    1024 x 1024 x 64\n",
            "  Maximum sizes of each dimension of a grid:     2147483647 x 65535 x 65535\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example demonstrates a simple vector sum on the GPU and on the host.\n",
        " * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the\n",
        " * GPU. A 2D thread block and 2D grid are used. sumArraysOnHost sequentially\n",
        " * iterates through vector elements on the host.\n",
        " */\n",
        "\n",
        "void initialData(int *ip, const int size)\n",
        "{\n",
        "    int i;\n",
        "\n",
        "    for(i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (int)(rand() & 0xFF);\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(int *A, int *B, int *C, const int nx,\n",
        "                     const int ny)\n",
        "{\n",
        "    int *ia = A;\n",
        "    int *ib = B;\n",
        "    int *ic = C;\n",
        "\n",
        "    for (int iy = 0; iy < ny; iy++)\n",
        "    {\n",
        "        for (int ix = 0; ix < nx; ix++)\n",
        "        {\n",
        "            ic[ix] = ia[ix] + ib[ix];\n",
        "\n",
        "        }\n",
        "\n",
        "        ia += nx;\n",
        "        ib += nx;\n",
        "        ic += nx;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void checkResult(int *hostRef, int *gpuRef, const int N)\n",
        "{\n",
        "    bool match = 1;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (hostRef[i] != gpuRef[i])\n",
        "        {\n",
        "            match = 0;\n",
        "            printf(\"host %d gpu %d\\n\", hostRef[i], gpuRef[i]);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (match)\n",
        "        printf(\"Arrays match.\\n\\n\");\n",
        "    else\n",
        "        printf(\"Arrays do not match.\\n\\n\");\n",
        "}\n",
        "\n",
        "// grid 2D block 2D\n",
        "__global__ void sumMatrixOnGPU2D(int *MatA, int *MatB, int *MatC, int nx,\n",
        "                                 int ny)\n",
        "{\n",
        "    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    unsigned int idx = iy * nx + ix;\n",
        "\n",
        "    if (ix < nx && iy < ny)\n",
        "        MatC[idx] = MatA[idx] + MatB[idx];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx = 1 << 12;\n",
        "    int ny = 1 << 12;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(int);\n",
        "    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n",
        "\n",
        "    // malloc host memory\n",
        "    int *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (int *)malloc(nBytes);\n",
        "    h_B = (int *)malloc(nBytes);\n",
        "    hostRef = (int *)malloc(nBytes);\n",
        "    gpuRef = (int *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "    printf(\"Matrix initialization elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnHost elapsed %f sec\\n\", iElaps);\n",
        "\n",
        "    // malloc device global memory\n",
        "    int *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 64;\n",
        "    int dimy = 2;\n",
        "    dim3 block(dimx, dimy);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnGPU2D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnGPU2D <<<(%d,%d), (%d,%d)>>> elapsed %f sec\\n\", grid.x,\n",
        "           grid.y,\n",
        "           block.x, block.y, iElaps);\n",
        "    // check kernel error\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return (0);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXX6zM79wMnP",
        "outputId": "487ab95b-c5a7-4856-9e3e-1ddd6aa7b6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpcv5k8_cb/d58d873e-7d43-4d72-bc1d-86d4161b93ff.out Starting...\n",
            "Using Device 0: Tesla T4\n",
            "Matrix size: nx 4096 ny 4096\n",
            "Matrix initialization elapsed 0.944288 sec\n",
            "sumMatrixOnHost elapsed 0.098832 sec\n",
            "sumMatrixOnGPU2D <<<(64,2048), (64,2)>>> elapsed 0.000862 sec\n",
            "Arrays match.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simpleDivergence.cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * simpleDivergence demonstrates divergent code on the GPU and its impact on\n",
        " * performance and CUDA metrics.\n",
        " */\n",
        "\n",
        "__global__ void mathKernel1(float *c)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float ia, ib;\n",
        "    ia = ib = 0.0f;\n",
        "\n",
        "    if (tid % 2 == 0)\n",
        "    {\n",
        "        ia = 100.0f;\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        ib = 200.0f;\n",
        "    }\n",
        "\n",
        "    c[tid] = ia + ib;\n",
        "}\n",
        "\n",
        "__global__ void mathKernel2(float *c)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float ia, ib;\n",
        "    ia = ib = 0.0f;\n",
        "\n",
        "    if ((tid / warpSize) % 2 == 0)\n",
        "    {\n",
        "        ia = 100.0f;\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        ib = 200.0f;\n",
        "    }\n",
        "\n",
        "    c[tid] = ia + ib;\n",
        "}\n",
        "\n",
        "__global__ void mathKernel3(float *c)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float ia, ib;\n",
        "    ia = ib = 0.0f;\n",
        "\n",
        "    bool ipred = (tid % 2 == 0);\n",
        "\n",
        "    if (ipred)\n",
        "    {\n",
        "        ia = 100.0f;\n",
        "    }\n",
        "\n",
        "    if (!ipred)\n",
        "    {\n",
        "        ib = 200.0f;\n",
        "    }\n",
        "\n",
        "    c[tid] = ia + ib;\n",
        "}\n",
        "\n",
        "__global__ void mathKernel4(float *c)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float ia, ib;\n",
        "    ia = ib = 0.0f;\n",
        "\n",
        "    int itid = tid >> 5;\n",
        "\n",
        "    if (itid & 0x01 == 0)\n",
        "    {\n",
        "        ia = 100.0f;\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        ib = 200.0f;\n",
        "    }\n",
        "\n",
        "    c[tid] = ia + ib;\n",
        "}\n",
        "\n",
        "__global__ void warmingup(float *c)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float ia, ib;\n",
        "    ia = ib = 0.0f;\n",
        "\n",
        "    if ((tid / warpSize) % 2 == 0)\n",
        "    {\n",
        "        ia = 100.0f;\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        ib = 200.0f;\n",
        "    }\n",
        "\n",
        "    c[tid] = ia + ib;\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s using Device %d: %s\\n\", argv[0], dev, deviceProp.name);\n",
        "\n",
        "    // set up data size\n",
        "    int size = 64;\n",
        "    int blocksize = 64;\n",
        "\n",
        "    if(argc > 1) blocksize = atoi(argv[1]);\n",
        "\n",
        "    if(argc > 2) size      = atoi(argv[2]);\n",
        "\n",
        "    printf(\"Data size %d \", size);\n",
        "\n",
        "    // set up execution configuration\n",
        "    dim3 block (blocksize, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"Execution Configure (block %d grid %d)\\n\", block.x, grid.x);\n",
        "\n",
        "    // allocate gpu memory\n",
        "    float *d_C;\n",
        "    size_t nBytes = size * sizeof(float);\n",
        "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
        "\n",
        "    // run a warmup kernel to remove overhead\n",
        "    double iStart, iElaps;\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    warmingup<<<grid, block>>>(d_C);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"warmup      <<< %4d %4d >>> elapsed %f sec \\n\", grid.x, block.x,\n",
        "           iElaps );\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // run kernel 1\n",
        "    iStart = seconds();\n",
        "    mathKernel1<<<grid, block>>>(d_C);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"mathKernel1 <<< %4d %4d >>> elapsed %f sec \\n\", grid.x, block.x,\n",
        "           iElaps );\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // run kernel 3\n",
        "    iStart = seconds();\n",
        "    mathKernel2<<<grid, block>>>(d_C);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"mathKernel2 <<< %4d %4d >>> elapsed %f sec \\n\", grid.x, block.x,\n",
        "           iElaps );\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // run kernel 3\n",
        "    iStart = seconds();\n",
        "    mathKernel3<<<grid, block>>>(d_C);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"mathKernel3 <<< %4d %4d >>> elapsed %f sec \\n\", grid.x, block.x,\n",
        "           iElaps);\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // run kernel 4\n",
        "    iStart = seconds();\n",
        "    mathKernel4<<<grid, block>>>(d_C);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"mathKernel4 <<< %4d %4d >>> elapsed %f sec \\n\", grid.x, block.x,\n",
        "           iElaps);\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // free gpu memory and reset divece\n",
        "    CHECK(cudaFree(d_C));\n",
        "    CHECK(cudaDeviceReset());\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWJya8r9Hw8y",
        "outputId": "da46c5e7-3306-4ceb-c906-4bada86152c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simpleDivergence.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc simpleDivergence.cu -o simpleDivergence"
      ],
      "metadata": {
        "id": "nVcV1QaIIEKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./simpleDivergence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJWlUyWSIpWo",
        "outputId": "d19d8983-9fac-44eb-a9e7-cd9c4fe37e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./simpleDivergence using Device 0: Tesla T4\n",
            "Data size 64 Execution Configure (block 64 grid 1)\n",
            "warmup      <<<    1   64 >>> elapsed 0.000033 sec \n",
            "mathKernel1 <<<    1   64 >>> elapsed 0.000012 sec \n",
            "mathKernel2 <<<    1   64 >>> elapsed 0.000011 sec \n",
            "mathKernel3 <<<    1   64 >>> elapsed 0.000011 sec \n",
            "mathKernel4 <<<    1   64 >>> elapsed 0.000011 sec \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nv-nsight-cu-cli ./simpleDivergence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F79bkvS8JHNG",
        "outputId": "7a6683fc-f253-47a9-8530-cc51c040b851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 4218 (/content/simpleDivergence)\n",
            "/content/./simpleDivergence using Device 0: Tesla T4\n",
            "Data size 64 Execution Configure (block 64 grid 1)\n",
            "==PROF== Profiling \"warmingup(float *)\" - 0: 0%....50%....100% - 8 passes\n",
            "warmup      <<<    1   64 >>> elapsed 0.000000 sec \n",
            "==PROF== Profiling \"mathKernel1(float *)\" - 1: 0%....50%....100% - 8 passes\n",
            "mathKernel1 <<<    1   64 >>> elapsed 0.000000 sec \n",
            "==PROF== Profiling \"mathKernel2(float *)\" - 2: 0%....50%....100% - 8 passes\n",
            "mathKernel2 <<<    1   64 >>> elapsed 0.000000 sec \n",
            "==PROF== Profiling \"mathKernel3(float *)\" - 3: 0%....50%....100% - 8 passes\n",
            "mathKernel3 <<<    1   64 >>> elapsed 0.000000 sec \n",
            "==PROF== Profiling \"mathKernel4(float *)\" - 4: 0%....50%....100% - 8 passes\n",
            "mathKernel4 <<<    1   64 >>> elapsed 0.000000 sec \n",
            "==PROF== Disconnected from process 4218\n",
            "[4218] simpleDivergence@127.0.0.1\n",
            "  warmingup(float *), 2023-Nov-28 13:42:36, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.64\n",
            "    SM Frequency                                                             cycle/usecond                         540.27\n",
            "    Elapsed Cycles                                                                   cycle                          1,833\n",
            "    Memory [%]                                                                           %                           0.63\n",
            "    DRAM Throughput                                                                      %                           0.05\n",
            "    Duration                                                                       usecond                           3.39\n",
            "    L1/TEX Cache Throughput                                                              %                          25.73\n",
            "    L2 Cache Throughput                                                                  %                           0.63\n",
            "    SM Active Cycles                                                                 cycle                          16.32\n",
            "    Compute (SM) [%]                                                                     %                           0.02\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                         64\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                             64\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             64\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                           6.23\n",
            "    Achieved Active Warps Per SM                                                      warp                           1.99\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads    \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  mathKernel1(float *), 2023-Nov-28 13:42:36, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.64\n",
            "    SM Frequency                                                             cycle/usecond                         545.94\n",
            "    Elapsed Cycles                                                                   cycle                          1,748\n",
            "    Memory [%]                                                                           %                           0.65\n",
            "    DRAM Throughput                                                                      %                           0.05\n",
            "    Duration                                                                       usecond                           3.20\n",
            "    L1/TEX Cache Throughput                                                              %                          29.22\n",
            "    L2 Cache Throughput                                                                  %                           0.65\n",
            "    SM Active Cycles                                                                 cycle                          14.38\n",
            "    Compute (SM) [%]                                                                     %                           0.01\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                         64\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                             64\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             64\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                           6.23\n",
            "    Achieved Active Warps Per SM                                                      warp                           1.99\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads    \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  mathKernel2(float *), 2023-Nov-28 13:42:36, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.72\n",
            "    SM Frequency                                                             cycle/usecond                         545.36\n",
            "    Elapsed Cycles                                                                   cycle                          1,833\n",
            "    Memory [%]                                                                           %                           0.63\n",
            "    DRAM Throughput                                                                      %                           0.05\n",
            "    Duration                                                                       usecond                           3.36\n",
            "    L1/TEX Cache Throughput                                                              %                          25.69\n",
            "    L2 Cache Throughput                                                                  %                           0.63\n",
            "    SM Active Cycles                                                                 cycle                          16.35\n",
            "    Compute (SM) [%]                                                                     %                           0.02\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                         64\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                             64\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             64\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                           6.23\n",
            "    Achieved Active Warps Per SM                                                      warp                           1.99\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads    \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  mathKernel3(float *), 2023-Nov-28 13:42:36, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.64\n",
            "    SM Frequency                                                             cycle/usecond                         547.62\n",
            "    Elapsed Cycles                                                                   cycle                          1,753\n",
            "    Memory [%]                                                                           %                           0.61\n",
            "    DRAM Throughput                                                                      %                           0.05\n",
            "    Duration                                                                       usecond                           3.20\n",
            "    L1/TEX Cache Throughput                                                              %                          29.32\n",
            "    L2 Cache Throughput                                                                  %                           0.61\n",
            "    SM Active Cycles                                                                 cycle                          14.32\n",
            "    Compute (SM) [%]                                                                     %                           0.01\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                         64\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                             64\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             64\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                           6.23\n",
            "    Achieved Active Warps Per SM                                                      warp                           1.99\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads    \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  mathKernel4(float *), 2023-Nov-28 13:42:36, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.64\n",
            "    SM Frequency                                                             cycle/usecond                         542.94\n",
            "    Elapsed Cycles                                                                   cycle                          1,738\n",
            "    Memory [%]                                                                           %                           0.61\n",
            "    DRAM Throughput                                                                      %                           0.05\n",
            "    Duration                                                                       usecond                           3.20\n",
            "    L1/TEX Cache Throughput                                                              %                          29.95\n",
            "    L2 Cache Throughput                                                                  %                           0.61\n",
            "    SM Active Cycles                                                                 cycle                          14.03\n",
            "    Compute (SM) [%]                                                                     %                           0.01\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                         64\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                             64\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                             64\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                           6.23\n",
            "    Achieved Active Warps Per SM                                                      warp                           1.99\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads    \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sumMatrix.cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This example implements matrix element-wise addition on the host and GPU.\n",
        " * sumMatrixOnHost iterates over the rows and columns of each matrix, adding\n",
        " * elements from A and B together and storing the results in C. The current\n",
        " * offset in each matrix is stored using pointer arithmetic. sumMatrixOnGPU2D\n",
        " * implements the same logic, but using CUDA threads to process each matrix.\n",
        " */\n",
        "\n",
        "void initialData(float *ip, const int size)\n",
        "{\n",
        "    int i;\n",
        "\n",
        "    for(i = 0; i < size; i++)\n",
        "    {\n",
        "        ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny)\n",
        "{\n",
        "    float *ia = A;\n",
        "    float *ib = B;\n",
        "    float *ic = C;\n",
        "\n",
        "    for (int iy = 0; iy < ny; iy++)\n",
        "    {\n",
        "        for (int ix = 0; ix < nx; ix++)\n",
        "        {\n",
        "            ic[ix] = ia[ix] + ib[ix];\n",
        "        }\n",
        "\n",
        "        ia += nx;\n",
        "        ib += nx;\n",
        "        ic += nx;\n",
        "    }\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
        "{\n",
        "    double epsilon = 1.0E-8;\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
        "        {\n",
        "            printf(\"host %f gpu %f \", hostRef[i], gpuRef[i]);\n",
        "            printf(\"Arrays do not match.\\n\\n\");\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// grid 2D block 2D\n",
        "__global__ void sumMatrixOnGPU2D(float *A, float *B, float *C, int NX, int NY)\n",
        "{\n",
        "    unsigned int ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    unsigned int iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    unsigned int idx = iy * NX + ix;\n",
        "\n",
        "    if (ix < NX && iy < NY)\n",
        "    {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // set up data size of matrix\n",
        "    int nx = 1 << 14;\n",
        "    int ny = 1 << 14;\n",
        "\n",
        "    int nxy = nx * ny;\n",
        "    int nBytes = nxy * sizeof(float);\n",
        "\n",
        "    // malloc host memory\n",
        "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
        "    h_A = (float *)malloc(nBytes);\n",
        "    h_B = (float *)malloc(nBytes);\n",
        "    hostRef = (float *)malloc(nBytes);\n",
        "    gpuRef = (float *)malloc(nBytes);\n",
        "\n",
        "    // initialize data at host side\n",
        "    double iStart = seconds();\n",
        "    initialData(h_A, nxy);\n",
        "    initialData(h_B, nxy);\n",
        "    double iElaps = seconds() - iStart;\n",
        "\n",
        "    memset(hostRef, 0, nBytes);\n",
        "    memset(gpuRef, 0, nBytes);\n",
        "\n",
        "    // add matrix at host side for result checks\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnHost (h_A, h_B, hostRef, nx, ny);\n",
        "    iElaps = seconds() - iStart;\n",
        "\n",
        "    // malloc device global memory\n",
        "    float *d_MatA, *d_MatB, *d_MatC;\n",
        "    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n",
        "    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n",
        "\n",
        "    // transfer data from host to device\n",
        "    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // invoke kernel at host side\n",
        "    int dimx = 32;\n",
        "    int dimy = 32;\n",
        "\n",
        "    if(argc > 2)\n",
        "    {\n",
        "        dimx = atoi(argv[1]);\n",
        "        dimy = atoi(argv[2]);\n",
        "    }\n",
        "\n",
        "    dim3 block(dimx, dimy);\n",
        "    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n",
        "\n",
        "    // execute the kernel\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    sumMatrixOnGPU2D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"sumMatrixOnGPU2D <<<(%d,%d), (%d,%d)>>> elapsed %f ms\\n\", grid.x,\n",
        "           grid.y,\n",
        "           block.x, block.y, iElaps);\n",
        "    CHECK(cudaGetLastError());\n",
        "\n",
        "    // copy kernel result back to host side\n",
        "    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // check device results\n",
        "    checkResult(hostRef, gpuRef, nxy);\n",
        "\n",
        "    // free device global memory\n",
        "    CHECK(cudaFree(d_MatA));\n",
        "    CHECK(cudaFree(d_MatB));\n",
        "    CHECK(cudaFree(d_MatC));\n",
        "\n",
        "    // free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(hostRef);\n",
        "    free(gpuRef);\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x1xDL5xKjya",
        "outputId": "2cb97e90-47be-4e6b-8eb5-bdf1da638b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sumMatrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc sumMatrix.cu -o sumMatrix"
      ],
      "metadata": {
        "id": "4pXQ4_oLKuCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./sumMatrix 32 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N29NixKMLAJ_",
        "outputId": "cdbc2fd3-7d9b-4307-879b-d04c8bf8d16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumMatrixOnGPU2D <<<(512,512), (32,32)>>> elapsed 0.013445 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./sumMatrix 32 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqwOcuKOPop2",
        "outputId": "fcef02fe-5eab-4a8b-d33f-ca04d6c2794b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumMatrixOnGPU2D <<<(512,1024), (32,16)>>> elapsed 0.012837 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./sumMatrix 16 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu947o4lP2Go",
        "outputId": "e7425b8c-e289-4d79-c08f-ba7fbeebd9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumMatrixOnGPU2D <<<(1024,512), (16,32)>>> elapsed 0.013657 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./sumMatrix 16 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlMcw8WiQIq_",
        "outputId": "1d36018d-25fe-4342-8710-93967d6d2961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumMatrixOnGPU2D <<<(1024,1024), (16,16)>>> elapsed 0.013098 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./sumMatrix 128 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGXP8XSiTPEq",
        "outputId": "af568bbe-9d9b-4f4f-a7f2-7414c20e6a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumMatrixOnGPU2D <<<(128,8192), (128,2)>>> elapsed 0.012493 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./sumMatrix 256 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3nwPaoOTX-n",
        "outputId": "f90f60d5-2f1d-440a-e717-eec498caeef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sumMatrixOnGPU2D <<<(64,2048), (256,8)>>> elapsed 0.000004 ms\n",
            "Error: sumMatrix.cu:141, code: 9, reason: invalid configuration argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nestedHelloWorld.cu\n",
        "#include \"../common/common.h\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "/*\n",
        " * A simple example of nested kernel launches from the GPU. Each thread displays\n",
        " * its information when execution begins, and also diagnostics when the next\n",
        " * lowest nesting layer completes.\n",
        " */\n",
        "\n",
        "__global__ void nestedHelloWorld(int const iSize, int iDepth)\n",
        "{\n",
        "    int tid = threadIdx.x;\n",
        "    printf(\"Recursion=%d: Hello World from thread %d block %d\\n\", iDepth, tid,\n",
        "           blockIdx.x);\n",
        "\n",
        "    // condition to stop recursive execution\n",
        "    if (iSize == 1) return;\n",
        "\n",
        "    // reduce block size to half\n",
        "    int nthreads = iSize >> 1;\n",
        "\n",
        "    // thread 0 launches child grid recursively\n",
        "    if(tid == 0 && nthreads > 0)\n",
        "    {\n",
        "        int blocks = (nthreads + blockDim.x - 1) / blockDim.x;\n",
        "        nestedHelloWorld<<<blocks, blockDim.x>>>(nthreads, ++iDepth);\n",
        "        printf(\"-------> nested execution depth: %d\\n\", iDepth);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    int size = 8;\n",
        "    int blocksize = 8;   // initial block size\n",
        "    int igrid = 1;\n",
        "\n",
        "    if(argc > 1)\n",
        "    {\n",
        "        igrid = atoi(argv[1]);\n",
        "        size = igrid * blocksize;\n",
        "    }\n",
        "\n",
        "    dim3 block (blocksize, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"%s Execution Configuration: grid %d block %d\\n\", argv[0], grid.x,\n",
        "           block.x);\n",
        "\n",
        "    nestedHelloWorld<<<grid, block>>>(block.x, 0);\n",
        "\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaDeviceReset());\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjA7FQmGeySK",
        "outputId": "6b140708-5666-47c0-9859-0e7294d679f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nestedHelloWorld.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -rdc=true -lcudadevrt nestedHelloWorld.cu -o nestedHelloWorld"
      ],
      "metadata": {
        "id": "mbp-18ZIgUcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./nestedHelloWorld 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz6bK3mzhBgT",
        "outputId": "2284770d-9e7c-45dd-a0e2-b1ffe9c8d278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./nestedHelloWorld Execution Configuration: grid 2 block 8\n",
            "Recursion=0: Hello World from thread 0 block 1\n",
            "Recursion=0: Hello World from thread 1 block 1\n",
            "Recursion=0: Hello World from thread 2 block 1\n",
            "Recursion=0: Hello World from thread 3 block 1\n",
            "Recursion=0: Hello World from thread 4 block 1\n",
            "Recursion=0: Hello World from thread 5 block 1\n",
            "Recursion=0: Hello World from thread 6 block 1\n",
            "Recursion=0: Hello World from thread 7 block 1\n",
            "Recursion=0: Hello World from thread 0 block 0\n",
            "Recursion=0: Hello World from thread 1 block 0\n",
            "Recursion=0: Hello World from thread 2 block 0\n",
            "Recursion=0: Hello World from thread 3 block 0\n",
            "Recursion=0: Hello World from thread 4 block 0\n",
            "Recursion=0: Hello World from thread 5 block 0\n",
            "Recursion=0: Hello World from thread 6 block 0\n",
            "Recursion=0: Hello World from thread 7 block 0\n",
            "-------> nested execution depth: 1\n",
            "-------> nested execution depth: 1\n",
            "Recursion=1: Hello World from thread 0 block 0\n",
            "Recursion=1: Hello World from thread 1 block 0\n",
            "Recursion=1: Hello World from thread 2 block 0\n",
            "Recursion=1: Hello World from thread 3 block 0\n",
            "Recursion=1: Hello World from thread 4 block 0\n",
            "Recursion=1: Hello World from thread 5 block 0\n",
            "Recursion=1: Hello World from thread 6 block 0\n",
            "Recursion=1: Hello World from thread 7 block 0\n",
            "Recursion=1: Hello World from thread 0 block 0\n",
            "Recursion=1: Hello World from thread 1 block 0\n",
            "Recursion=1: Hello World from thread 2 block 0\n",
            "Recursion=1: Hello World from thread 3 block 0\n",
            "Recursion=1: Hello World from thread 4 block 0\n",
            "Recursion=1: Hello World from thread 5 block 0\n",
            "Recursion=1: Hello World from thread 6 block 0\n",
            "Recursion=1: Hello World from thread 7 block 0\n",
            "-------> nested execution depth: 2\n",
            "-------> nested execution depth: 2\n",
            "Recursion=2: Hello World from thread 0 block 0\n",
            "Recursion=2: Hello World from thread 1 block 0\n",
            "Recursion=2: Hello World from thread 2 block 0\n",
            "Recursion=2: Hello World from thread 3 block 0\n",
            "Recursion=2: Hello World from thread 4 block 0\n",
            "Recursion=2: Hello World from thread 5 block 0\n",
            "Recursion=2: Hello World from thread 6 block 0\n",
            "Recursion=2: Hello World from thread 7 block 0\n",
            "Recursion=2: Hello World from thread 0 block 0\n",
            "Recursion=2: Hello World from thread 1 block 0\n",
            "Recursion=2: Hello World from thread 2 block 0\n",
            "Recursion=2: Hello World from thread 3 block 0\n",
            "Recursion=2: Hello World from thread 4 block 0\n",
            "Recursion=2: Hello World from thread 5 block 0\n",
            "Recursion=2: Hello World from thread 6 block 0\n",
            "Recursion=2: Hello World from thread 7 block 0\n",
            "-------> nested execution depth: 3\n",
            "Recursion=3: Hello World from thread 0 block 0\n",
            "Recursion=3: Hello World from thread 1 block 0\n",
            "Recursion=3: Hello World from thread 2 block 0\n",
            "Recursion=3: Hello World from thread 3 block 0\n",
            "Recursion=3: Hello World from thread 4 block 0\n",
            "Recursion=3: Hello World from thread 5 block 0\n",
            "Recursion=3: Hello World from thread 6 block 0\n",
            "Recursion=3: Hello World from thread 7 block 0\n",
            "-------> nested execution depth: 3\n",
            "Recursion=3: Hello World from thread 0 block 0\n",
            "Recursion=3: Hello World from thread 1 block 0\n",
            "Recursion=3: Hello World from thread 2 block 0\n",
            "Recursion=3: Hello World from thread 3 block 0\n",
            "Recursion=3: Hello World from thread 4 block 0\n",
            "Recursion=3: Hello World from thread 5 block 0\n",
            "Recursion=3: Hello World from thread 6 block 0\n",
            "Recursion=3: Hello World from thread 7 block 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./nestedHelloWorld 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgcr85c0hA-N",
        "outputId": "eed70ced-4c4f-455a-daba-ed6d58177463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./nestedHelloWorld Execution Configuration: grid 2 block 8\n",
            "Recursion=0: Hello World from thread 0 block 1\n",
            "Recursion=0: Hello World from thread 1 block 1\n",
            "Recursion=0: Hello World from thread 2 block 1\n",
            "Recursion=0: Hello World from thread 3 block 1\n",
            "Recursion=0: Hello World from thread 4 block 1\n",
            "Recursion=0: Hello World from thread 5 block 1\n",
            "Recursion=0: Hello World from thread 6 block 1\n",
            "Recursion=0: Hello World from thread 7 block 1\n",
            "Recursion=0: Hello World from thread 0 block 0\n",
            "Recursion=0: Hello World from thread 1 block 0\n",
            "Recursion=0: Hello World from thread 2 block 0\n",
            "Recursion=0: Hello World from thread 3 block 0\n",
            "Recursion=0: Hello World from thread 4 block 0\n",
            "Recursion=0: Hello World from thread 5 block 0\n",
            "Recursion=0: Hello World from thread 6 block 0\n",
            "Recursion=0: Hello World from thread 7 block 0\n",
            "-------> nested execution depth: 1\n",
            "-------> nested execution depth: 1\n",
            "Recursion=1: Hello World from thread 0 block 0\n",
            "Recursion=1: Hello World from thread 1 block 0\n",
            "Recursion=1: Hello World from thread 2 block 0\n",
            "Recursion=1: Hello World from thread 3 block 0\n",
            "Recursion=1: Hello World from thread 0 block 0\n",
            "Recursion=1: Hello World from thread 1 block 0\n",
            "Recursion=1: Hello World from thread 2 block 0\n",
            "Recursion=1: Hello World from thread 3 block 0\n",
            "-------> nested execution depth: 2\n",
            "Recursion=2: Hello World from thread 0 block 0\n",
            "Recursion=2: Hello World from thread 1 block 0\n",
            "-------> nested execution depth: 2\n",
            "Recursion=2: Hello World from thread 0 block 0\n",
            "Recursion=2: Hello World from thread 1 block 0\n",
            "-------> nested execution depth: 3\n",
            "Recursion=3: Hello World from thread 0 block 0\n",
            "-------> nested execution depth: 3\n",
            "Recursion=3: Hello World from thread 0 block 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nestedReduce.cu\n",
        "#include \"../common/common.h\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#define LOG 0\n",
        "\n",
        "/*\n",
        " * An implementation of parallel reduction using nested kernel launches from\n",
        " * CUDA kernels.\n",
        " */\n",
        "\n",
        "// Recursive Implementation of Interleaved Pair Approach\n",
        "int cpuRecursiveReduce(int *data, int const size)\n",
        "{\n",
        "    // stop condition\n",
        "    if (size == 1) return data[0];\n",
        "\n",
        "    // renew the stride\n",
        "    int const stride = size / 2;\n",
        "\n",
        "    // in-place reduction\n",
        "    for (int i = 0; i < stride; i++)\n",
        "    {\n",
        "        data[i] += data[i + stride];\n",
        "    }\n",
        "\n",
        "    // call recursively\n",
        "    return cpuRecursiveReduce(data, stride);\n",
        "}\n",
        "\n",
        "// Neighbored Pair Implementation with divergence\n",
        "__global__ void reduceNeighbored (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((tid % (2 * stride)) == 0)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void gpuRecursiveReduce (int *g_idata, int *g_odata,\n",
        "                                    unsigned int isize)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "    int *odata = &g_odata[blockIdx.x];\n",
        "\n",
        "    // stop condition\n",
        "    if (isize == 2 && tid == 0)\n",
        "    {\n",
        "        g_odata[blockIdx.x] = idata[0] + idata[1];\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // nested invocation\n",
        "    int istride = isize >> 1;\n",
        "\n",
        "    if(istride > 1 && tid < istride)\n",
        "    {\n",
        "        // in place reduction\n",
        "        idata[tid] += idata[tid + istride];\n",
        "    }\n",
        "\n",
        "    // sync at block level\n",
        "    __syncthreads();\n",
        "\n",
        "    // nested invocation to generate child grids\n",
        "    if(tid == 0)\n",
        "    {\n",
        "        gpuRecursiveReduce<<<1, istride>>>(idata, odata, istride);\n",
        "\n",
        "        // sync all child grids launched in this block\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    // sync at block level again\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "// main from here\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0, gpu_sum;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s starting reduction at \", argv[0]);\n",
        "    printf(\"device %d: %s \", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    bool bResult = false;\n",
        "\n",
        "    // set up execution configuration\n",
        "    int nblock  = 2048;\n",
        "    int nthread = 512;   // initial block size\n",
        "\n",
        "    if(argc > 1)\n",
        "    {\n",
        "        nblock = atoi(argv[1]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    if(argc > 2)\n",
        "    {\n",
        "        nthread = atoi(argv[2]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    int size = nblock * nthread; // total number of elements to reduceNeighbored\n",
        "\n",
        "    dim3 block (nthread, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"array %d grid %d block %d\\n\", size, grid.x, block.x);\n",
        "\n",
        "    // allocate host memory\n",
        "    size_t bytes = size * sizeof(int);\n",
        "    int *h_idata = (int *) malloc(bytes);\n",
        "    int *h_odata = (int *) malloc(grid.x * sizeof(int));\n",
        "    int *tmp     = (int *) malloc(bytes);\n",
        "\n",
        "    // initialize the array\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        h_idata[i] = (int)( rand() & 0xFF );\n",
        "        h_idata[i] = 1;\n",
        "    }\n",
        "\n",
        "    memcpy (tmp, h_idata, bytes);\n",
        "\n",
        "    // allocate device memory\n",
        "    int *d_idata = NULL;\n",
        "    int *d_odata = NULL;\n",
        "    CHECK(cudaMalloc((void **) &d_idata, bytes));\n",
        "    CHECK(cudaMalloc((void **) &d_odata, grid.x * sizeof(int)));\n",
        "\n",
        "    double iStart, iElaps;\n",
        "\n",
        "    // cpu recursive reduction\n",
        "    iStart = seconds();\n",
        "    int cpu_sum = cpuRecursiveReduce (tmp, size);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"cpu reduce\\t\\telapsed %f sec cpu_sum: %d\\n\", iElaps, cpu_sum);\n",
        "\n",
        "    // gpu reduceNeighbored\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    reduceNeighbored<<<grid, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Neighbored\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // gpu nested reduce kernel\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    gpuRecursiveReduce<<<grid, block>>>(d_idata, d_odata, block.x);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu nested\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block %d>>>\\n\",\n",
        "           iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // free host memory\n",
        "    free(h_idata);\n",
        "    free(h_odata);\n",
        "\n",
        "    // free device memory\n",
        "    CHECK(cudaFree(d_idata));\n",
        "    CHECK(cudaFree(d_odata));\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    // check the results\n",
        "    bResult = (gpu_sum == cpu_sum);\n",
        "\n",
        "    if(!bResult) printf(\"Test failed!\\n\");\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlHoVjzynKEF",
        "outputId": "459b5417-8a2f-481a-8d23-7fdd8ddd048b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing nestedReduce.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -rdc=true -lcudadevrt nestedReduce.cu -o nestedReduce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y51v6r68nJs8",
        "outputId": "c45b24bd-de6f-4749-bed0-f74d039c69a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mnestedReduce.cu(94)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1444-D: function \u001b[01m\"cudaDeviceSynchronize\"\u001b[0m\u001b[32m\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here\u001b[0m was declared deprecated (\"\u001b[01mUse of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\u001b[0m\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./nestedReduce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N57r_kUpnktK",
        "outputId": "927c8e86-6009-43f5-f8e8-d533dc0db483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./nestedReduce starting reduction at device 0: Tesla T4 array 1048576 grid 2048 block 512\n",
            "cpu reduce\t\telapsed 0.003079 sec cpu_sum: 1048576\n",
            "gpu Neighbored\t\telapsed 0.000302 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n",
            "gpu nested\t\telapsed 0.054148 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nestedReduceNosync.cu\n",
        "#include \"../common/common.h\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#define LOG 0\n",
        "\n",
        "/*\n",
        " * An implementation of parallel reduction using nested kernel launches from\n",
        " * CUDA kernels. This version adds optimizations on to the work in\n",
        " * nestedReduce.cu.\n",
        " */\n",
        "\n",
        "// Recursive Implementation of Interleaved Pair Approach\n",
        "int cpuRecursiveReduce(int *data, int const size)\n",
        "{\n",
        "    // stop condition\n",
        "    if (size == 1) return data[0];\n",
        "\n",
        "    // renew the stride\n",
        "    int const stride = size / 2;\n",
        "\n",
        "    // in-place reduction\n",
        "    for (int i = 0; i < stride; i++)\n",
        "    {\n",
        "        data[i] += data[i + stride];\n",
        "    }\n",
        "\n",
        "    // call recursively\n",
        "    return cpuRecursiveReduce(data, stride);\n",
        "}\n",
        "\n",
        "// Neighbored Pair Implementation with divergence\n",
        "__global__ void reduceNeighbored (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((tid % (2 * stride)) == 0)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void gpuRecursiveReduce (int *g_idata, int *g_odata,\n",
        "                                    unsigned int isize)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "    int *odata = &g_odata[blockIdx.x];\n",
        "\n",
        "    // stop condition\n",
        "    if (isize == 2 && tid == 0)\n",
        "    {\n",
        "        g_odata[blockIdx.x] = idata[0] + idata[1];\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // nested invocation\n",
        "    int istride = isize >> 1;\n",
        "\n",
        "    if(istride > 1 && tid < istride)\n",
        "    {\n",
        "        // in place reduction\n",
        "        idata[tid] += idata[tid + istride];\n",
        "    }\n",
        "\n",
        "    // sync at block level\n",
        "    __syncthreads();\n",
        "\n",
        "    // nested invocation to generate child grids\n",
        "    if(tid == 0)\n",
        "    {\n",
        "        gpuRecursiveReduce<<<1, istride>>>(idata, odata, istride);\n",
        "\n",
        "        // sync all child grids launched in this block\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    // sync at block level again\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "__global__ void gpuRecursiveReduceNosync (int *g_idata, int *g_odata,\n",
        "        unsigned int isize)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "    int *odata = &g_odata[blockIdx.x];\n",
        "\n",
        "    // stop condition\n",
        "    if (isize == 2 && tid == 0)\n",
        "    {\n",
        "        g_odata[blockIdx.x] = idata[0] + idata[1];\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // nested invoke\n",
        "    int istride = isize >> 1;\n",
        "\n",
        "    if(istride > 1 && tid < istride)\n",
        "    {\n",
        "        idata[tid] += idata[tid + istride];\n",
        "\n",
        "        if(tid == 0)\n",
        "        {\n",
        "            gpuRecursiveReduceNosync<<<1, istride>>>(idata, odata, istride);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// main from here\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0, gpu_sum;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s starting reduction at \", argv[0]);\n",
        "    printf(\"device %d: %s \", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    bool bResult = false;\n",
        "\n",
        "    // set up execution configuration\n",
        "    int nblock  = 2048;\n",
        "    int nthread = 512;   // initial block size\n",
        "\n",
        "    if(argc > 1)\n",
        "    {\n",
        "        nblock = atoi(argv[1]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    if(argc > 2)\n",
        "    {\n",
        "        nthread = atoi(argv[2]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    int size = nblock * nthread; // total number of elements to reduceNeighbored\n",
        "\n",
        "    dim3 block (nthread, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"array %d grid %d block %d\\n\", size, grid.x, block.x);\n",
        "\n",
        "    // allocate host memory\n",
        "    size_t bytes = size * sizeof(int);\n",
        "    int *h_idata = (int *) malloc(bytes);\n",
        "    int *h_odata = (int *) malloc(grid.x * sizeof(int));\n",
        "    int *tmp     = (int *) malloc(bytes);\n",
        "\n",
        "    // initialize the array\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        h_idata[i] = (int)( rand() & 0xFF );\n",
        "        h_idata[i] = 1;\n",
        "    }\n",
        "\n",
        "    memcpy (tmp, h_idata, bytes);\n",
        "\n",
        "    // allocate device memory\n",
        "    int *d_idata = NULL;\n",
        "    int *d_odata = NULL;\n",
        "    CHECK(cudaMalloc((void **) &d_idata, bytes));\n",
        "    CHECK(cudaMalloc((void **) &d_odata, grid.x * sizeof(int)));\n",
        "\n",
        "    double iStart, iElaps;\n",
        "\n",
        "    // cpu recursive reduction\n",
        "    iStart = seconds();\n",
        "    int cpu_sum = cpuRecursiveReduce (tmp, size);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"cpu reduce\\t\\telapsed %f sec cpu_sum: %d\\n\", iElaps, cpu_sum);\n",
        "\n",
        "    // gpu reduceNeighbored\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    reduceNeighbored<<<grid, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Neighbored\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // gpu nested reduce kernel\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    gpuRecursiveReduce<<<grid, block>>>(d_idata, d_odata, block.x);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu nested\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block %d>>>\\n\",\n",
        "           iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // gpu nested reduce kernel without synchronization\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    gpuRecursiveReduceNosync<<<grid, block>>>(d_idata, d_odata, block.x);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu nestedNosyn\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // free host memory\n",
        "    free(h_idata);\n",
        "    free(h_odata);\n",
        "\n",
        "    // free device memory\n",
        "    CHECK(cudaFree(d_idata));\n",
        "    CHECK(cudaFree(d_odata));\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    // check the results\n",
        "    bResult = (gpu_sum == cpu_sum);\n",
        "\n",
        "    if(!bResult) printf(\"Test failed!\\n\");\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYMZVka_oTgy",
        "outputId": "7814d01b-247e-4022-df1a-55c9b4e109bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing nestedReduceNosync.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -rdc=true -lcudadevrt nestedReduceNosync.cu -o nestedReduceNosync"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjNtCNDmokNs",
        "outputId": "05626ee5-f33e-4653-bc44-a120de3b6d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mnestedReduceNosync.cu(95)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1444-D: function \u001b[01m\"cudaDeviceSynchronize\"\u001b[0m\u001b[32m\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here\u001b[0m was declared deprecated (\"\u001b[01mUse of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\u001b[0m\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./nestedReduceNosync"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k40LPGU1o1IE",
        "outputId": "ec3098ed-a2d3-41f8-c0ea-39733b3074e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./nestedReduceNosync starting reduction at device 0: Tesla T4 array 1048576 grid 2048 block 512\n",
            "cpu reduce\t\telapsed 0.003012 sec cpu_sum: 1048576\n",
            "gpu Neighbored\t\telapsed 0.000300 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n",
            "gpu nested\t\telapsed 0.055035 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n",
            "gpu nestedNosyn\t\telapsed 0.025942 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nestedReduce2.cu\n",
        "#include \"../common/common.h\"\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#define LOG 0\n",
        "\n",
        "/*\n",
        " * An implementation of parallel reduction using nested kernel launches from\n",
        " * CUDA kernels. This version adds optimizations on to the work in\n",
        " * nestedReduce.cu.\n",
        " */\n",
        "\n",
        "// Recursive Implementation of Interleaved Pair Approach\n",
        "int cpuRecursiveReduce(int *data, int const size)\n",
        "{\n",
        "    // stop condition\n",
        "    if (size == 1) return data[0];\n",
        "\n",
        "    // renew the stride\n",
        "    int const stride = size / 2;\n",
        "\n",
        "    // in-place reduction\n",
        "    for (int i = 0; i < stride; i++)\n",
        "    {\n",
        "        data[i] += data[i + stride];\n",
        "    }\n",
        "\n",
        "    // call recursively\n",
        "    return cpuRecursiveReduce(data, stride);\n",
        "}\n",
        "\n",
        "// Neighbored Pair Implementation with divergence\n",
        "__global__ void reduceNeighbored (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((tid % (2 * stride)) == 0)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void gpuRecursiveReduce (int *g_idata, int *g_odata,\n",
        "                                    unsigned int isize)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "    int *odata = &g_odata[blockIdx.x];\n",
        "\n",
        "    // stop condition\n",
        "    if (isize == 2 && tid == 0)\n",
        "    {\n",
        "        g_odata[blockIdx.x] = idata[0] + idata[1];\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // nested invocation\n",
        "    int istride = isize >> 1;\n",
        "\n",
        "    if(istride > 1 && tid < istride)\n",
        "    {\n",
        "        // in place reduction\n",
        "        idata[tid] += idata[tid + istride];\n",
        "    }\n",
        "\n",
        "    // sync at block level\n",
        "    __syncthreads();\n",
        "\n",
        "    // nested invocation to generate child grids\n",
        "    if(tid == 0)\n",
        "    {\n",
        "        gpuRecursiveReduce<<<1, istride>>>(idata, odata, istride);\n",
        "\n",
        "        // sync all child grids launched in this block\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    // sync at block level again\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "__global__ void gpuRecursiveReduceNosync (int *g_idata, int *g_odata,\n",
        "        unsigned int isize)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "    int *odata = &g_odata[blockIdx.x];\n",
        "\n",
        "    // stop condition\n",
        "    if (isize == 2 && tid == 0)\n",
        "    {\n",
        "        g_odata[blockIdx.x] = idata[0] + idata[1];\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // nested invoke\n",
        "    int istride = isize >> 1;\n",
        "\n",
        "    if(istride > 1 && tid < istride)\n",
        "    {\n",
        "        idata[tid] += idata[tid + istride];\n",
        "\n",
        "        if(tid == 0)\n",
        "        {\n",
        "            gpuRecursiveReduceNosync<<<1, istride>>>(idata, odata, istride);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void gpuRecursiveReduce2(int *g_idata, int *g_odata, int iStride,\n",
        "                                    int const iDim)\n",
        "{\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * iDim;\n",
        "\n",
        "    // stop condition\n",
        "    if (iStride == 1 && threadIdx.x == 0)\n",
        "    {\n",
        "        g_odata[blockIdx.x] = idata[0] + idata[1];\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // in place reduction\n",
        "    idata[threadIdx.x] += idata[threadIdx.x + iStride];\n",
        "\n",
        "    // nested invocation to generate child grids\n",
        "    if(threadIdx.x == 0 && blockIdx.x == 0)\n",
        "    {\n",
        "        gpuRecursiveReduce2<<<1, iStride / 2>>>(g_idata, g_odata,\n",
        "                iStride / 2, iDim);\n",
        "    }\n",
        "}\n",
        "\n",
        "// main from here\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0, gpu_sum;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s starting reduction at \", argv[0]);\n",
        "    printf(\"device %d: %s \", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    bool bResult = false;\n",
        "\n",
        "    // set up execution configuration\n",
        "    int nblock  = 2048;\n",
        "    int nthread = 512;   // initial block size\n",
        "\n",
        "    if(argc > 1)\n",
        "    {\n",
        "        nblock = atoi(argv[1]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    if(argc > 2)\n",
        "    {\n",
        "        nthread = atoi(argv[2]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    int size = nblock * nthread; // total number of elements to reduceNeighbored\n",
        "\n",
        "    dim3 block (nthread, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"array %d grid %d block %d\\n\", size, grid.x, block.x);\n",
        "\n",
        "    // allocate host memory\n",
        "    size_t bytes = size * sizeof(int);\n",
        "    int *h_idata = (int *) malloc(bytes);\n",
        "    int *h_odata = (int *) malloc(grid.x * sizeof(int));\n",
        "    int *tmp     = (int *) malloc(bytes);\n",
        "\n",
        "    // initialize the array\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        h_idata[i] = (int)( rand() & 0xFF );\n",
        "        h_idata[i] = 1;\n",
        "    }\n",
        "\n",
        "    memcpy (tmp, h_idata, bytes);\n",
        "\n",
        "    // allocate device memory\n",
        "    int *d_idata = NULL;\n",
        "    int *d_odata = NULL;\n",
        "    CHECK(cudaMalloc((void **) &d_idata, bytes));\n",
        "    CHECK(cudaMalloc((void **) &d_odata, grid.x * sizeof(int)));\n",
        "\n",
        "    double iStart, iElaps;\n",
        "\n",
        "    // cpu recursive reduction\n",
        "    iStart = seconds();\n",
        "    int cpu_sum = cpuRecursiveReduce (tmp, size);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"cpu reduce\\t\\telapsed %f sec cpu_sum: %d\\n\", iElaps, cpu_sum);\n",
        "\n",
        "    // gpu reduceNeighbored\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    reduceNeighbored<<<grid, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Neighbored\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // gpu nested reduce kernel\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    gpuRecursiveReduce<<<grid, block>>>(d_idata, d_odata, block.x);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu nested\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block %d>>>\\n\",\n",
        "           iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // gpu nested reduce kernel without synchronization\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    gpuRecursiveReduceNosync<<<grid, block>>>(d_idata, d_odata, block.x);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu nestedNosyn\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    iStart = seconds();\n",
        "    gpuRecursiveReduce2<<<grid, block.x/2>>>(d_idata, d_odata, block.x/2,\n",
        "            block.x);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu nested2\\t\\telapsed %f sec gpu_sum: %d <<<grid %d block %d>>>\\n\",\n",
        "           iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // free host memory\n",
        "    free(h_idata);\n",
        "    free(h_odata);\n",
        "\n",
        "    // free device memory\n",
        "    CHECK(cudaFree(d_idata));\n",
        "    CHECK(cudaFree(d_odata));\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    // check the results\n",
        "    bResult = (gpu_sum == cpu_sum);\n",
        "\n",
        "    if(!bResult) printf(\"Test failed!\\n\");\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qiviQi1rhia",
        "outputId": "13448b79-eb7b-4228-a5e8-68bf9057f3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nestedReduce2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -rdc=true -lcudadevrt nestedReduce2.cu -o nestedReduced2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo2VWIULroAA",
        "outputId": "11f80a9c-40d9-44c3-9491-c8d0e544e63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mnestedReduce2.cu(95)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1444-D: function \u001b[01m\"cudaDeviceSynchronize\"\u001b[0m\u001b[32m\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(139): here\u001b[0m was declared deprecated (\"\u001b[01mUse of cudaDeviceSynchronize from device code is deprecated. Moreover, such use will cause this module to fail to load on sm_90+ devices. If calls to cudaDeviceSynchronize from device code cannot be removed for older devices at this time, you may guard them with __CUDA_ARCH__ macros to remove them only for sm_90+ devices, making sure to generate code for compute_90 for the macros to take effect. Note that this mitigation will no longer work when support for cudaDeviceSynchronize from device code is eventually dropped for all devices. Disable this warning with -D__CDPRT_SUPPRESS_SYNC_DEPRECATION_WARNING.\u001b[0m\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./nestedReduced2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQMt8Ku7r-0F",
        "outputId": "5cd182b5-0e70-47a8-b892-296b2d8b77d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./nestedReduced2 starting reduction at device 0: Tesla T4 array 1048576 grid 2048 block 512\n",
            "cpu reduce\t\telapsed 0.003462 sec cpu_sum: 1048576\n",
            "gpu Neighbored\t\telapsed 0.000304 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n",
            "gpu nested\t\telapsed 0.054695 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n",
            "gpu nestedNosyn\t\telapsed 0.025842 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n",
            "gpu nested2\t\telapsed 0.001350 sec gpu_sum: 1048576 <<<grid 2048 block 512>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * This code implements the interleaved and neighbor-paired approaches to\n",
        " * parallel reduction in CUDA. For this example, the sum operation is used. A\n",
        " * variety of optimizations on parallel reduction aimed at reducing divergence\n",
        " * are also demonstrated, such as unrolling.\n",
        " */\n",
        "\n",
        "// Recursive Implementation of Interleaved Pair Approach\n",
        "int recursiveReduce(int *data, int const size)\n",
        "{\n",
        "    // terminate check\n",
        "    if (size == 1) return data[0];\n",
        "\n",
        "    // renew the stride\n",
        "    int const stride = size / 2;\n",
        "\n",
        "    // in-place reduction\n",
        "    for (int i = 0; i < stride; i++)\n",
        "    {\n",
        "        data[i] += data[i + stride];\n",
        "    }\n",
        "\n",
        "    // call recursively\n",
        "    return recursiveReduce(data, stride);\n",
        "}\n",
        "\n",
        "// Neighbored Pair Implementation with divergence\n",
        "__global__ void reduceNeighbored (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if (idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((tid % (2 * stride)) == 0)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "// Neighbored Pair Implementation with less divergence\n",
        "__global__ void reduceNeighboredLess (int *g_idata, int *g_odata,\n",
        "                                      unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if(idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2)\n",
        "    {\n",
        "        // convert tid into local array index\n",
        "        int index = 2 * stride * tid;\n",
        "\n",
        "        if (index < blockDim.x)\n",
        "        {\n",
        "            idata[index] += idata[index + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "// Interleaved Pair Implementation with less divergence\n",
        "__global__ void reduceInterleaved (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // boundary check\n",
        "    if(idx >= n) return;\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceUnrolling2 (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 2;\n",
        "\n",
        "    // unrolling 2\n",
        "    if (idx + blockDim.x < n) g_idata[idx] += g_idata[idx + blockDim.x];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceUnrolling4 (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 4;\n",
        "\n",
        "    // unrolling 4\n",
        "    if (idx + 3 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceUnrolling8 (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 8;\n",
        "\n",
        "    // unrolling 8\n",
        "    if (idx + 7 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        int b1 = g_idata[idx + 4 * blockDim.x];\n",
        "        int b2 = g_idata[idx + 5 * blockDim.x];\n",
        "        int b3 = g_idata[idx + 6 * blockDim.x];\n",
        "        int b4 = g_idata[idx + 7 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void reduceUnrolling8UsingFor (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 8;\n",
        "\n",
        "    // unrolling 8\n",
        "    if (idx + 7 * blockDim.x < n)\n",
        "    {\n",
        "        int *ptr = g_idata + idx;\n",
        "        int tmp = 0;\n",
        "\n",
        "        // Increment tmp 8 times with values strided by blockDim.x\n",
        "        for (int i = 0; i < 8; i++) {\n",
        "            tmp += *ptr; ptr += blockDim.x;\n",
        "        }\n",
        "\n",
        "        g_idata[idx] = tmp;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__ void reduceUnrolling16 (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 16 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 16;\n",
        "\n",
        "    // unrolling 16\n",
        "    if (idx + 15 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        int b1 = g_idata[idx + 4 * blockDim.x];\n",
        "        int b2 = g_idata[idx + 5 * blockDim.x];\n",
        "        int b3 = g_idata[idx + 6 * blockDim.x];\n",
        "        int b4 = g_idata[idx + 7 * blockDim.x];\n",
        "        int c1 = g_idata[idx + 8*blockDim.x];\n",
        "        int c2 = g_idata[idx + 9*blockDim.x];\n",
        "        int c3 = g_idata[idx + 10 * blockDim.x];\n",
        "        int c4 = g_idata[idx + 11 * blockDim.x];\n",
        "        int d1 = g_idata[idx + 12 * blockDim.x];\n",
        "        int d2 = g_idata[idx + 13 * blockDim.x];\n",
        "        int d3 = g_idata[idx + 14 * blockDim.x];\n",
        "        int d4 = g_idata[idx + 15 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4 + c1 + c2 + c3+c4 + d1+d2+d3+d4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceUnrollWarps8 (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 8;\n",
        "\n",
        "    // unrolling 8\n",
        "    if (idx + 7 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        int b1 = g_idata[idx + 4 * blockDim.x];\n",
        "        int b2 = g_idata[idx + 5 * blockDim.x];\n",
        "        int b3 = g_idata[idx + 6 * blockDim.x];\n",
        "        int b4 = g_idata[idx + 7 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 32; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vmem = idata;\n",
        "        vmem[tid] += vmem[tid + 32];\n",
        "        vmem[tid] += vmem[tid + 16];\n",
        "        vmem[tid] += vmem[tid +  8];\n",
        "        vmem[tid] += vmem[tid +  4];\n",
        "        vmem[tid] += vmem[tid +  2];\n",
        "        vmem[tid] += vmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceCompleteUnrollWarps8 (int *g_idata, int *g_odata,\n",
        "        unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 8;\n",
        "\n",
        "    // unrolling 8\n",
        "    if (idx + 7 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        int b1 = g_idata[idx + 4 * blockDim.x];\n",
        "        int b2 = g_idata[idx + 5 * blockDim.x];\n",
        "        int b3 = g_idata[idx + 6 * blockDim.x];\n",
        "        int b4 = g_idata[idx + 7 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction and complete unroll\n",
        "    if (blockDim.x >= 1024 && tid < 512) idata[tid] += idata[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 512 && tid < 256) idata[tid] += idata[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 256 && tid < 128) idata[tid] += idata[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (blockDim.x >= 128 && tid < 64) idata[tid] += idata[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = idata;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "template <unsigned int iBlockSize>\n",
        "__global__ void reduceCompleteUnroll(int *g_idata, int *g_odata,\n",
        "                                     unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 8;\n",
        "\n",
        "    // unrolling 8\n",
        "    if (idx + 7 * blockDim.x < n)\n",
        "    {\n",
        "        int a1 = g_idata[idx];\n",
        "        int a2 = g_idata[idx + blockDim.x];\n",
        "        int a3 = g_idata[idx + 2 * blockDim.x];\n",
        "        int a4 = g_idata[idx + 3 * blockDim.x];\n",
        "        int b1 = g_idata[idx + 4 * blockDim.x];\n",
        "        int b2 = g_idata[idx + 5 * blockDim.x];\n",
        "        int b3 = g_idata[idx + 6 * blockDim.x];\n",
        "        int b4 = g_idata[idx + 7 * blockDim.x];\n",
        "        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction and complete unroll\n",
        "    if (iBlockSize >= 1024 && tid < 512) idata[tid] += idata[tid + 512];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (iBlockSize >= 512 && tid < 256)  idata[tid] += idata[tid + 256];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (iBlockSize >= 256 && tid < 128)  idata[tid] += idata[tid + 128];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (iBlockSize >= 128 && tid < 64)   idata[tid] += idata[tid + 64];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // unrolling warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = idata;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    // write result for this block to global mem\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "__global__ void reduceUnrollWarps (int *g_idata, int *g_odata, unsigned int n)\n",
        "{\n",
        "    // set thread ID\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;\n",
        "\n",
        "    // convert global data pointer to the local pointer of this block\n",
        "    int *idata = g_idata + blockIdx.x * blockDim.x * 2;\n",
        "\n",
        "    // unrolling 2\n",
        "    if (idx + blockDim.x < n) g_idata[idx] += g_idata[idx + blockDim.x];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // in-place reduction in global memory\n",
        "    for (int stride = blockDim.x / 2; stride > 32; stride >>= 1)\n",
        "    {\n",
        "        if (tid < stride)\n",
        "        {\n",
        "            idata[tid] += idata[tid + stride];\n",
        "        }\n",
        "\n",
        "        // synchronize within threadblock\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // unrolling last warp\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        volatile int *vsmem = idata;\n",
        "        vsmem[tid] += vsmem[tid + 32];\n",
        "        vsmem[tid] += vsmem[tid + 16];\n",
        "        vsmem[tid] += vsmem[tid +  8];\n",
        "        vsmem[tid] += vsmem[tid +  4];\n",
        "        vsmem[tid] += vsmem[tid +  2];\n",
        "        vsmem[tid] += vsmem[tid +  1];\n",
        "    }\n",
        "\n",
        "    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"%s starting reduction at \", argv[0]);\n",
        "    printf(\"device %d: %s \", dev, deviceProp.name);\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    bool bResult = false;\n",
        "\n",
        "    // initialization\n",
        "    int size = 1 << 24; // total number of elements to reduce\n",
        "    printf(\"    with array size %d  \", size);\n",
        "\n",
        "    // execution configuration\n",
        "    int blocksize = 512;   // initial block size\n",
        "\n",
        "    if(argc > 1)\n",
        "    {\n",
        "        blocksize = atoi(argv[1]);   // block size from command line argument\n",
        "    }\n",
        "\n",
        "    dim3 block (blocksize, 1);\n",
        "    dim3 grid  ((size + block.x - 1) / block.x, 1);\n",
        "    printf(\"grid %d block %d\\n\", grid.x, block.x);\n",
        "\n",
        "    // allocate host memory\n",
        "    size_t bytes = size * sizeof(int);\n",
        "    int *h_idata = (int *) malloc(bytes);\n",
        "    int *h_odata = (int *) malloc(grid.x * sizeof(int));\n",
        "    int *tmp     = (int *) malloc(bytes);\n",
        "\n",
        "    // initialize the array\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        // mask off high 2 bytes to force max number to 255\n",
        "        h_idata[i] = (int)( rand() & 0xFF );\n",
        "    }\n",
        "\n",
        "    memcpy (tmp, h_idata, bytes);\n",
        "\n",
        "    double iStart, iElaps;\n",
        "    int gpu_sum = 0;\n",
        "\n",
        "    // allocate device memory\n",
        "    int *d_idata = NULL;\n",
        "    int *d_odata = NULL;\n",
        "    CHECK(cudaMalloc((void **) &d_idata, bytes));\n",
        "    CHECK(cudaMalloc((void **) &d_odata, grid.x * sizeof(int)));\n",
        "\n",
        "    // cpu reduction\n",
        "    iStart = seconds();\n",
        "    int cpu_sum = recursiveReduce (tmp, size);\n",
        "    iElaps = seconds() - iStart;\n",
        "    printf(\"cpu reduce      elapsed %f sec cpu_sum: %d\\n\", iElaps, cpu_sum);\n",
        "\n",
        "    // kernel 1: reduceNeighbored\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceNeighbored<<<grid, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Neighbored  elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // kernel 2: reduceNeighbored with less divergence\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceNeighboredLess<<<grid, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Neighbored2 elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // kernel 3: reduceInterleaved\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceInterleaved<<<grid, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Interleaved elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x, block.x);\n",
        "\n",
        "    // kernel 4: reduceUnrolling2\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceUnrolling2<<<grid.x / 2, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 2 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 2; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Unrolling2  elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 2, block.x);\n",
        "\n",
        "    // kernel 5: reduceUnrolling4\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceUnrolling4<<<grid.x / 4, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Unrolling4  elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 4, block.x);\n",
        "\n",
        "    // kernel 6: reduceUnrolling8\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceUnrolling8<<<grid.x / 8, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 8 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 8; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Unrolling8  elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 8, block.x);\n",
        "\n",
        "    for (int i = 0; i < grid.x / 16; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "        // kernel 6: reduceUnrolling8-2\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceUnrolling8UsingFor<<<grid.x / 8, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 8 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 8; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Unrolling8For  elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 8, block.x);\n",
        "\n",
        "    // kernel 7: reduceUnrolling16\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceUnrolling16<<<grid.x / 16, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 16 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 16; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Unrolling16 elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 16, block.x);\n",
        "\n",
        "\n",
        "    // kernel 8: reduceUnrollWarps8\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceUnrollWarps8<<<grid.x / 8, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 8 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 8; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu UnrollWarp8 elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 8, block.x);\n",
        "\n",
        "\n",
        "    // kernel 9: reduceCompleteUnrollWarsp8\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "    reduceCompleteUnrollWarps8<<<grid.x / 8, block>>>(d_idata, d_odata, size);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 8 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 8; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Cmptnroll8  elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 8, block.x);\n",
        "\n",
        "    // kernel 9: reduceCompleteUnroll\n",
        "    CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iStart = seconds();\n",
        "\n",
        "    switch (blocksize)\n",
        "    {\n",
        "    case 1024:\n",
        "        reduceCompleteUnroll<1024><<<grid.x / 8, block>>>(d_idata, d_odata,\n",
        "                size);\n",
        "        break;\n",
        "\n",
        "    case 512:\n",
        "        reduceCompleteUnroll<512><<<grid.x / 8, block>>>(d_idata, d_odata,\n",
        "                size);\n",
        "        break;\n",
        "\n",
        "    case 256:\n",
        "        reduceCompleteUnroll<256><<<grid.x / 8, block>>>(d_idata, d_odata,\n",
        "                size);\n",
        "        break;\n",
        "\n",
        "    case 128:\n",
        "        reduceCompleteUnroll<128><<<grid.x / 8, block>>>(d_idata, d_odata,\n",
        "                size);\n",
        "        break;\n",
        "\n",
        "    case 64:\n",
        "        reduceCompleteUnroll<64><<<grid.x / 8, block>>>(d_idata, d_odata, size);\n",
        "        break;\n",
        "    }\n",
        "\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    iElaps = seconds() - iStart;\n",
        "    CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 8 * sizeof(int),\n",
        "                     cudaMemcpyDeviceToHost));\n",
        "\n",
        "    gpu_sum = 0;\n",
        "\n",
        "    for (int i = 0; i < grid.x / 8; i++) gpu_sum += h_odata[i];\n",
        "\n",
        "    printf(\"gpu Cmptnroll   elapsed %f sec gpu_sum: %d <<<grid %d block \"\n",
        "           \"%d>>>\\n\", iElaps, gpu_sum, grid.x / 8, block.x);\n",
        "\n",
        "    // free host memory\n",
        "    free(h_idata);\n",
        "    free(h_odata);\n",
        "\n",
        "    // free device memory\n",
        "    CHECK(cudaFree(d_idata));\n",
        "    CHECK(cudaFree(d_odata));\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "\n",
        "    // check the results\n",
        "    bResult = (gpu_sum == cpu_sum);\n",
        "\n",
        "    if(!bResult) printf(\"Test failed!\\n\");\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI0nmeXn08xj",
        "outputId": "a0f28e6a-1d46-40ae-b02b-73c7509cdd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpk9wexiin/40f4b0f7-979c-4546-bda7-a3a5fa0e3b0b.out starting reduction at device 0: Tesla T4     with array size 16777216  grid 32768 block 512\n",
            "cpu reduce      elapsed 0.053846 sec cpu_sum: 2139353471\n",
            "gpu Neighbored  elapsed 0.003345 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>\n",
            "gpu Neighbored2 elapsed 0.001850 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>\n",
            "gpu Interleaved elapsed 0.001662 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>\n",
            "gpu Unrolling2  elapsed 0.000928 sec gpu_sum: 2139353471 <<<grid 16384 block 512>>>\n",
            "gpu Unrolling4  elapsed 0.000508 sec gpu_sum: 2139353471 <<<grid 8192 block 512>>>\n",
            "gpu Unrolling8  elapsed 0.000315 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>\n",
            "gpu Unrolling8For  elapsed 0.000314 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>\n",
            "gpu Unrolling16 elapsed 0.000294 sec gpu_sum: 2139353471 <<<grid 2048 block 512>>>\n",
            "gpu UnrollWarp8 elapsed 0.000353 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>\n",
            "gpu Cmptnroll8  elapsed 0.000348 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>\n",
            "gpu Cmptnroll   elapsed 0.000350 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include \"../common/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * An example of using CUDA's memory copy API to transfer data to and from the\n",
        " * device. In this case, cudaMalloc is used to allocate memory on the GPU and\n",
        " * cudaMemcpy is used to transfer the contents of host memory to an array\n",
        " * allocated using cudaMalloc. Host memory is allocated using cudaMallocHost to\n",
        " * create a page-locked host array.\n",
        " */\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    // set up device\n",
        "    int dev = 0;\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "\n",
        "    // memory size\n",
        "    unsigned int isize = 1 << 22;\n",
        "    unsigned int nbytes = isize * sizeof(float);\n",
        "\n",
        "    // get device information\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "\n",
        "    if (!deviceProp.canMapHostMemory)\n",
        "    {\n",
        "        printf(\"Device %d does not support mapping CPU host memory!\\n\", dev);\n",
        "        CHECK(cudaDeviceReset());\n",
        "        exit(EXIT_SUCCESS);\n",
        "    }\n",
        "\n",
        "    printf(\"%s starting at \", argv[0]);\n",
        "    printf(\"device %d: %s memory size %d nbyte %5.2fMB canMap %d\\n\", dev,\n",
        "           deviceProp.name, isize, nbytes / (1024.0f * 1024.0f),\n",
        "           deviceProp.canMapHostMemory);\n",
        "\n",
        "    // allocate pinned host memory\n",
        "    float *h_a;\n",
        "    CHECK(cudaMallocHost ((float **)&h_a, nbytes));\n",
        "\n",
        "    // allocate device memory\n",
        "    float *d_a;\n",
        "    CHECK(cudaMalloc((float **)&d_a, nbytes));\n",
        "\n",
        "    // initialize host memory\n",
        "    memset(h_a, 0, nbytes);\n",
        "\n",
        "    for (int i = 0; i < isize; i++) h_a[i] = 100.10f;\n",
        "\n",
        "    // transfer data from the host to the device\n",
        "    CHECK(cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // transfer data from the device to the host\n",
        "    CHECK(cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // free memory\n",
        "    CHECK(cudaFree(d_a));\n",
        "    CHECK(cudaFreeHost(h_a));\n",
        "\n",
        "    // reset device\n",
        "    CHECK(cudaDeviceReset());\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "id": "OSBEnwBD-wYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16b01c5-5538-4fef-bc40-6447cf5ab27f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpsy33xm22/6fe3c997-c86c-49d6-a3e5-c6e8ff24d3c3.out starting at device 0: Tesla T4 memory size 4194304 nbyte 16.00MB canMap 1\n",
            "\n"
          ]
        }
      ]
    }
  ]
}